[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Authors",
    "section": "",
    "text": "This project is the collaborative effort of two Master of City Planning students specializing in Sustainable Transportation and Infrastructure Planning. Driven by a commitment to creating people-centered transit solutions, the authors bring a data-driven and equity-focused approach to addressing pressing urban mobility challenges\n\n\n\nRiya Saini\n(She/ Her)\nMaster of City Planning Student | Sustainable Transportation & Infrastructure Planning\nRiya Saini is a Master of City Planning student at UPenn with a focus on urban mobility. She is passionate about transportation accessibility and the impact of transit on public health. Riya has experience in transportation modeling, trip generation mode choice modeling, and generating visually stunning graphs analyzing transit and vehicle accessibility across the US.\n\n\n\n\n\nVarun Bhakhri\n(He/ Him)\nMaster of City Planning Student | Sustainable Transportation & Infrastructure Planning\nVarun Bhakhri is a Master of City Planning student at UPenn with a focus on sustainable transportation led development. He is passionate about improving transit operations through comprehensive geospatial analysis and urban design. Varun has experience in transit planning, multimodal integration and transit oriented design in projects in India and the US."
  },
  {
    "objectID": "social_vulnerability.html",
    "href": "social_vulnerability.html",
    "title": "Transit Vulnerability",
    "section": "",
    "text": "In order to address disparities in transportation access and prioritize vulnerable communities, we developed a Transit Vulnerability Index (TVI) for Philadelphia. This index integrates a variety of demographic, transit accessibility, and environmental factors to identify areas most in need of impactful transit interventions. Inspired by existing methodologies, such as the work by Duan et al. (2016), our TVI emphasizes factors such as median income, racial demographics, workforce commuting patterns, access to vehicles, and mode choice. Additionally, we incorporate the mean travel time to work, which directly impacts daily commute times, and the city’s heat vulnerability index, which identifies areas at risk during extreme heat events.\nDrawing from the findings in Duan et al. (2016), the TVI recognizes that vulnerability is not solely about exposure to transportation challenges but is also shaped by social and environmental factors, such as income inequality and heat stress, which exacerbate the mobility challenges faced by disadvantaged populations. We focus on these factors at the census tract level.\nNote: we opted to not go lower than census tract level to reduce margins of error in our result.\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport cenpy as cny\nimport pygris\nfrom pygris import tracts\nimport numpy as np\nimport altair as alt\nimport seaborn as sns\nimport shapely\nfrom shapely.geometry import Point\n\n\n\n\nCode\n# Establish connection to the API\nacs = cny.remote.APIConnection(\"ACSDT5Y2023\")\n\n# Define variables and query data for Philadelphia County\nvariables = [\n    \"B19013_001E\",  # Median household income in the past 12 months\n    \"B01003_001E\",  # Total population\n    \"B11001_001E\",  # Total households\n    \"B08006_001E\",  # Total workers (commuting population)    \n    \"B02001_002E\",  # White alone\n    \"B02001_003E\",  # Black or African American alone\n    \"B08013_001E\",  # Mean travel time to work\n    \"B08201_002E\",  # No vehicles available\n    \"B08201_003E\",  # 1 vehicle available\n    \"B08201_004E\",  # 2 vehicles available\n    \"B08201_005E\",  # 3 vehicles available\n    \"B08201_006E\",  # 4 or more vehicles available\n    \"B08006_002E\",  # Car, truck, or van – drove alone\n    \"B08006_003E\",  # Car, truck, or van – carpooled\n    \"B08006_008E\",  # Public transportation (excluding taxicab)\n    \"B08006_015E\",  # Walked\n    \"B08006_017E\",  # Bicycle\n    \"B08006_018E\"   # Worked at home\n]\n\n# Query data\nacs_data = acs.query(\n    variables,\n    geo_unit=\"tract\",\n    geo_filter={\"state\": \"42\", \"county\": \"101\"})\n\n# Format GEOID\nacs_data['GEOID'] = (\n    acs_data['state'] +\n    acs_data['county'] +\n    acs_data['tract'])\n\n# Rename columns \nacs_data = acs_data.rename(columns={\n    'B19013_001E': 'Median_Income',\n    'B01003_001E': 'Total_Population',\n    'B11001_001E': 'Total_households',\n    'B08006_001E': 'Total_Workers',\n    'B02001_002E': 'White_Alone',\n    'B02001_003E': 'Black_Alone',\n    'B08013_001E': 'Mean_travel_time',\n    'B08201_002E': 'No_vehicles',\n    'B08201_003E': 'One_vehicle',\n    'B08201_004E': 'Two_vehicles',\n    'B08201_005E': 'Three_vehicles',\n    'B08201_006E': 'Four_or_more_vehicles',\n    'B08006_002E': 'Drove_Alone',\n    'B08006_003E': 'Carpooled',\n    'B08006_008E': 'Public_Transportation',\n    'B08006_015E': 'Walked',\n    'B08006_017E': 'Bicycle',\n    'B08006_018E': 'Worked_at_Home'})\n\n# Drop  columns\nacs_data = acs_data.drop(columns=['state', 'county','tract'])\n\n# Downloading census tract geometries for Philadelphia County\nphila_tracts = tracts(state=\"42\", county=\"101\", cb=True, year=2023)\n\n# Merge ACS data with geometries on GEOID\nacs_data = phila_tracts.merge(acs_data, on=\"GEOID\", how=\"left\")\n\n# Drop columns\nacs_data = acs_data[['GEOID', 'geometry', 'Median_Income', 'Total_Population', 'Total_households', 'Total_Workers',\n       'White_Alone', 'Black_Alone', 'Mean_travel_time', 'No_vehicles', 'One_vehicle', 'Two_vehicles', \n       'Three_vehicles', 'Four_or_more_vehicles', 'Drove_Alone', 'Carpooled',\n       'Public_Transportation', 'Walked', 'Bicycle', 'Worked_at_Home']]\n\nacs_data.head\n\n\n&lt;bound method NDFrame.head of            GEOID                                           geometry  \\\n0    42101001500  POLYGON ((-75.16558 39.94366, -75.16021 39.943...   \n1    42101001800  POLYGON ((-75.16620 39.94081, -75.16599 39.941...   \n2    42101002802  POLYGON ((-75.16735 39.92658, -75.16350 39.929...   \n3    42101004001  POLYGON ((-75.17002 39.92314, -75.16974 39.924...   \n4    42101006300  POLYGON ((-75.24686 39.91876, -75.23967 39.923...   \n..           ...                                                ...   \n403  42101000500  POLYGON ((-75.16506 39.95361, -75.16332 39.953...   \n404  42101027100  POLYGON ((-75.12749 40.05022, -75.12714 40.051...   \n405  42101000200  POLYGON ((-75.16269 39.95623, -75.16234 39.957...   \n406  42101013200  POLYGON ((-75.16096 39.96447, -75.16080 39.965...   \n407  42101032100  POLYGON ((-75.06511 40.01751, -75.06113 40.019...   \n\n    Median_Income Total_Population Total_households Total_Workers White_Alone  \\\n0          108378             3027             1486          1978        2213   \n1          121719             3285             1700          2289        2425   \n2           94427             5868             2432          3148        3525   \n3           82258             4118             2105          2662        3165   \n4           32997             4380             1696          1776          99   \n..            ...              ...              ...           ...         ...   \n403         68977             3292             1480          1591        1351   \n404         60255             2638             1053          1059         157   \n405         97256             3259             1742          1737        1490   \n406         44299             3450             1752          1584        1077   \n407         50409             3686             1629          1766        1466   \n\n    Black_Alone Mean_travel_time No_vehicles One_vehicle Two_vehicles  \\\n0           466            41280         570         758          119   \n1           443            46780         354        1098          165   \n2           319            60580         439        1545          422   \n3           135            70965         465        1112          498   \n4          3667            54680         652         799          133   \n..          ...              ...         ...         ...          ...   \n403        1105            30435        1031         396           53   \n404        1832            38785         165         411          433   \n405         264            31870         452        1108          155   \n406        1563            38195         793         655          252   \n407         757            49210         349        1066          148   \n\n    Three_vehicles Four_or_more_vehicles Drove_Alone Carpooled  \\\n0                5                    34         488       468   \n1               42                    41         716       687   \n2               26                     0        1199      1001   \n3               11                    19        1033       873   \n4               85                    27        1067      1001   \n..             ...                   ...         ...       ...   \n403              0                     0         314       314   \n404             35                     9         699       626   \n405              0                    27         404       283   \n406             52                     0         579       559   \n407             33                    33        1419      1103   \n\n    Public_Transportation Walked Bicycle Worked_at_Home  \n0                     372    377     565            982  \n1                     101    617     690            983  \n2                     618    253     857           1855  \n3                     743    193     483           1291  \n4                     586     84      29            751  \n..                    ...    ...     ...            ...  \n403                   454    444     359            721  \n404                   269      0      80            481  \n405                   249    503     549           1081  \n406                   461    193     328            529  \n407                   221     68      43            842  \n\n[408 rows x 20 columns]&gt;\n\n\n\n\n\nBased on the following maps and bar chart, We know that most workers in Philadelphia commute to work by car. The share of workers who take transit is significantly lower with the exception of North Philadelphia (Strawberry Mansion, Brewerytown). As we will see below, these neighborhoods also have some of the lowest median incomes in Philadelphia. The lack of capital often reduces such users to dependent riders, who will travel by transit regardless of the level of service. However, such riders are also worthy of improved level of service.\n\n\nCode\n# List of columns to plot\ncolumns_to_plot = ['Drove_Alone', 'Carpooled', 'Public_Transportation', 'Walked', 'Bicycle', 'Worked_at_Home']\nacs_data_melted = acs_data.melt(id_vars=['GEOID'], value_vars=columns_to_plot, \n                                var_name='Mode of Transportation', value_name='Count')\n\nbrush = alt.selection_interval()\n\n# Map1: Car use to work\nmap_plot = alt.Chart(acs_data).mark_geoshape().encode(\n    color=alt.Color('Drove_Alone:Q', scale=alt.Scale(scheme='blues')),\n    tooltip=['Drove_Alone:Q', 'Carpooled:Q', 'Public_Transportation:Q', 'Walked:Q', 'Bicycle:Q', 'Worked_at_Home:Q']\n).add_params(brush).properties(\n    width=200,\n    height=400,\n    title=\"Drove Alone to Work\")\n\n# Map2: Public Transportation use to work\nmap_plot1 = alt.Chart(acs_data).mark_geoshape().encode(\n    color=alt.Color('Public_Transportation:Q', scale=alt.Scale(scheme='reds')),\n    tooltip=['Drove_Alone:Q', 'Carpooled:Q', 'Public_Transportation:Q', 'Walked:Q', 'Bicycle:Q', 'Worked_at_Home:Q']\n).add_params(brush).properties(\n    width=200,\n    height=400,\n    title=\"Public Transportation to Work\")\n\n# Map3: Total mode of transportation\nbars = alt.Chart(acs_data_melted).mark_bar().encode(\n    x=alt.X('sum(Count):Q', title=\"Total Number of Workers by Mode of Transportation\"),\n    y=alt.Y('Mode of Transportation:N', title=\"Mode of Transportation\"),\n    color='Mode of Transportation:N'\n).transform_filter(\n    brush\n).properties(width=300)\n\n# Combine the maps\nchart = alt.vconcat(\n    alt.hconcat(map_plot, map_plot1), bars)\n\nchart\n\n\n\n\n\n\n\n\n\n\n\nFor our analysis, we aimed to examine key indicators that could influence a commuter’s mode choice. We first compared the total population and total number of workers in each census tract to identify where work-based trips are most likely to be generated. The two graphs show a strong correlation, indicating that areas with larger populations also tend to have a higher number of workers.\nIn 2023, the median household income in Philadelphia was $60,698, adjusted for inflation. However, when visualizing the data on the map, income disparities become apparent. High-income areas are concentrated in the Northwest and South, while the West, North, and Northeast regions fall below the median income threshold, highlighting areas of economic inequality.\nWe explored travel time across different census tracts to identify areas with greater mobility challenges. As expected, regions farther from Center City experience longer travel times. However, it is noteworthy that travel times for residents in North Philly and Lower West Philly are higher than those in other nearby areas, despite their proximity to the city center.\nOur map also reveals several key correlations. Areas with a majority of Black households tend to have a higher proportion of households without vehicles. Additionally, these areas generally exhibit lower median incomes, further indicating socioeconomic disparities that influence transportation needs and mode choice across the city.\n\n\nCode\n# Convert object columns to numeric, coercing errors into NaN (missing data)\ncols_to_convert = ['Median_Income', 'Total_Population', 'Total_Workers', 'White_Alone', 'Black_Alone',\n                   'Mean_travel_time', 'No_vehicles', 'One_vehicle', 'Two_vehicles', \n                   'Three_vehicles', 'Four_or_more_vehicles']\n\nfor col in cols_to_convert:\n    acs_data[col] = pd.to_numeric(acs_data[col], errors='coerce')\n    \n# Calculate the share of each race relative to the total population\nacs_data['White_Share'] = acs_data['White_Alone'] / acs_data['Total_Population']\nacs_data['Black_Share'] = acs_data['Black_Alone'] / acs_data['Total_Population']\n\n# Assign the race with the highest share\nacs_data['race_majority'] = acs_data[['White_Share', 'Black_Share']].idxmax(axis=1)\n\n# Clean data\nacs_data = acs_data[acs_data['Median_Income'] != -666666666]\n\n# Calculate Mean travel time in minutes\nacs_data['Average_travel_time'] = (acs_data['Mean_travel_time'] / acs_data['Total_Workers']).fillna(0).apply(lambda x: max(x, 0))\nacs_data['Average_travel_time'] = acs_data['Average_travel_time'].round(0).astype(int)\n\n\nC:\\Users\\USER\\miniforge3\\envs\\musa-550-fall-2023\\lib\\site-packages\\geopandas\\geodataframe.py:1538: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\nCode\n# Create a list of the columns to plot\ncolumns_to_plot = ['Total_Population', 'Total_Workers', 'Median_Income','Average_travel_time', 'No_vehicles', 'race_majority']\n\n# Set up a grid of subplots\nfig, axes = plt.subplots(3, 2, figsize=(8, 12)) \naxes = axes.flatten() \n\n# Loop through the columns to plot each one\nfor i, column in enumerate(columns_to_plot):\n    ax = axes[i]\n    acs_data.plot(column=column, ax=ax, legend=True, cmap='Blues')\n    ax.set_title(f\"{column}\")\n    ax.set_axis_off()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWhile researching a transit rider vulnerability matrix, we consistently found that exposure to heat was a critical factor. However, vulnerability to heat is closely correlated with several other variables, many of which we have already explored. As a result, we decided to incorporate the City’s Heat Vulnerability Index (HVI) to highlight areas most at risk of experiencing heat-related illness.\nThe Philadelphia Heat Vulnerability Index (HVI) captures key factors that contribute to the negative health effects of extreme heat events. The HVI rating is calculated by combining both exposure and sensitivity indicators:\n\nAge: The percentage of the population over 65 years old, as older individuals are more susceptible to heat-related illnesses.\nEducational Attainment: The percentage of individuals over 25 years old without a high school diploma, which can be linked to lower awareness of heat risks.\nLanguage Barrier: The percentage of limited English-speaking households, which can affect the ability to receive heat-related warnings and information.\nLow Socioeconomic Status: The percentage of the population living below the Federal Poverty Level, as economically disadvantaged individuals may have fewer resources to adapt to extreme heat.\nRace & Ethnicity: The percentage of the population identifying as non-white, as certain racial and ethnic groups may experience heightened vulnerability due to social and economic factors.\nSocial Isolation: The percentage of the population over 65 years old living alone, as socially isolated individuals may lack support during heat events.\nHealth Status: This factor considers the underlying health conditions that may exacerbate the effects of extreme heat, such as chronic diseases or disabilities that limit mobility or access to cooling resources.\n\n\n\nCode\nheat = pd.read_csv('data\\heat_vulnerability_ct.csv')\nheat.rename(columns={'geoid10': 'GEOID'}, inplace=True)\n\n# Merge the heat vulnerability data with acs data\nheat['GEOID'] = heat['GEOID'].astype(str)\nacs_data_index = acs_data.merge(heat, on='GEOID')\n\n# List of the columns to plot\ncolumns_to_plot = ['hsi_score','hei_score', 'hvi_score', 'n_veryhigh']\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 6))\naxes = axes.flatten() \n\n# Loop through the columns and plot each one\nfor i, column in enumerate(columns_to_plot):\n    ax = axes[i]\n    acs_data_index.plot(column=column, ax=ax, legend=True, cmap='Blues')\n    ax.set_title(f\"{column}\")\n    ax.set_axis_off()  \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe City’s Heat Vulnerability Index (HVI) incorporates several socio-demographic parameters to assess vulnerability, and for our Transit Vulnerability Index (TVI), we have integrated these factors with transit-specific parameters, such as travel time and lack of available household vehicles. However, by focusing solely on these three factors, we may overlook households that, although located far from transit routes and reliant on personal vehicles for commuting, still experience economic disadvantage. To address this gap, we reintroduced the income variable to better account for such populations.\nGiven that this assessment is focused primarily on transit, we assigned a value of 1 to both median income and HVI, and a value of 2 to commute time and access to vehicles, reflecting their relative importance in influencing transit vulnerability. Our final TVI, ranging from 0 to 6, provides a comprehensive vulnerability score for census tracts across Philadelphia. Notably, the map below highlights North Philadelphia, particularly Strawberry Mansion, and West Philadelphia, specifically Cobbs Creek, as areas where transit riders are particularly vulnerable.\n\n\nCode\n# Social Vulnerability Index (SVI)\n\nhighest_quartile_travel_time = acs_data_index['Average_travel_time'].quantile(0.75)\nhighest_quartile_no_vehicles = acs_data_index['No_vehicles'].quantile(0.75)\nlowest_quartile_median_income = acs_data_index['Median_Income'].quantile(0.25)\n\nacs_data_index['Social_Vulnerability_Index'] = 0\n\n# Assign points\nacs_data_index.loc[acs_data_index['n_veryhigh'] == 1, 'Social_Vulnerability_Index'] += 1\nacs_data_index.loc[acs_data_index['Median_Income'] &lt;= lowest_quartile_median_income, 'Social_Vulnerability_Index'] += 1\nacs_data_index.loc[acs_data_index['Average_travel_time'] &gt;= highest_quartile_travel_time, 'Social_Vulnerability_Index'] += 2\nacs_data_index.loc[acs_data_index['No_vehicles'] &gt;= highest_quartile_no_vehicles, 'Social_Vulnerability_Index'] += 2\nacs_data_index[['n_veryhigh', 'Average_travel_time', 'No_vehicles', 'Median_Income','Social_Vulnerability_Index']].head()\n\n# Create the first map plot (Choropleth map)\nmap_plot = alt.Chart(acs_data_index).mark_geoshape().encode(\n    color=alt.Color('Social_Vulnerability_Index:Q', scale=alt.Scale(scheme='blues')),\n    tooltip=['GEOID:N', 'n_veryhigh:Q', 'Average_travel_time:Q', 'No_vehicles:Q', 'Median_Income:Q','race_majority:N']\n).add_params(brush).properties(\n    width=500,\n    height=500)\n\nchart = alt.vconcat(map_plot)\nchart"
  },
  {
    "objectID": "social_vulnerability.html#transit-vulnerability-index",
    "href": "social_vulnerability.html#transit-vulnerability-index",
    "title": "Transit Vulnerability",
    "section": "",
    "text": "In order to address disparities in transportation access and prioritize vulnerable communities, we developed a Transit Vulnerability Index (TVI) for Philadelphia. This index integrates a variety of demographic, transit accessibility, and environmental factors to identify areas most in need of impactful transit interventions. Inspired by existing methodologies, such as the work by Duan et al. (2016), our TVI emphasizes factors such as median income, racial demographics, workforce commuting patterns, access to vehicles, and mode choice. Additionally, we incorporate the mean travel time to work, which directly impacts daily commute times, and the city’s heat vulnerability index, which identifies areas at risk during extreme heat events.\nDrawing from the findings in Duan et al. (2016), the TVI recognizes that vulnerability is not solely about exposure to transportation challenges but is also shaped by social and environmental factors, such as income inequality and heat stress, which exacerbate the mobility challenges faced by disadvantaged populations. We focus on these factors at the census tract level.\nNote: we opted to not go lower than census tract level to reduce margins of error in our result.\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport cenpy as cny\nimport pygris\nfrom pygris import tracts\nimport numpy as np\nimport altair as alt\nimport seaborn as sns\nimport shapely\nfrom shapely.geometry import Point\n\n\n\n\nCode\n# Establish connection to the API\nacs = cny.remote.APIConnection(\"ACSDT5Y2023\")\n\n# Define variables and query data for Philadelphia County\nvariables = [\n    \"B19013_001E\",  # Median household income in the past 12 months\n    \"B01003_001E\",  # Total population\n    \"B11001_001E\",  # Total households\n    \"B08006_001E\",  # Total workers (commuting population)    \n    \"B02001_002E\",  # White alone\n    \"B02001_003E\",  # Black or African American alone\n    \"B08013_001E\",  # Mean travel time to work\n    \"B08201_002E\",  # No vehicles available\n    \"B08201_003E\",  # 1 vehicle available\n    \"B08201_004E\",  # 2 vehicles available\n    \"B08201_005E\",  # 3 vehicles available\n    \"B08201_006E\",  # 4 or more vehicles available\n    \"B08006_002E\",  # Car, truck, or van – drove alone\n    \"B08006_003E\",  # Car, truck, or van – carpooled\n    \"B08006_008E\",  # Public transportation (excluding taxicab)\n    \"B08006_015E\",  # Walked\n    \"B08006_017E\",  # Bicycle\n    \"B08006_018E\"   # Worked at home\n]\n\n# Query data\nacs_data = acs.query(\n    variables,\n    geo_unit=\"tract\",\n    geo_filter={\"state\": \"42\", \"county\": \"101\"})\n\n# Format GEOID\nacs_data['GEOID'] = (\n    acs_data['state'] +\n    acs_data['county'] +\n    acs_data['tract'])\n\n# Rename columns \nacs_data = acs_data.rename(columns={\n    'B19013_001E': 'Median_Income',\n    'B01003_001E': 'Total_Population',\n    'B11001_001E': 'Total_households',\n    'B08006_001E': 'Total_Workers',\n    'B02001_002E': 'White_Alone',\n    'B02001_003E': 'Black_Alone',\n    'B08013_001E': 'Mean_travel_time',\n    'B08201_002E': 'No_vehicles',\n    'B08201_003E': 'One_vehicle',\n    'B08201_004E': 'Two_vehicles',\n    'B08201_005E': 'Three_vehicles',\n    'B08201_006E': 'Four_or_more_vehicles',\n    'B08006_002E': 'Drove_Alone',\n    'B08006_003E': 'Carpooled',\n    'B08006_008E': 'Public_Transportation',\n    'B08006_015E': 'Walked',\n    'B08006_017E': 'Bicycle',\n    'B08006_018E': 'Worked_at_Home'})\n\n# Drop  columns\nacs_data = acs_data.drop(columns=['state', 'county','tract'])\n\n# Downloading census tract geometries for Philadelphia County\nphila_tracts = tracts(state=\"42\", county=\"101\", cb=True, year=2023)\n\n# Merge ACS data with geometries on GEOID\nacs_data = phila_tracts.merge(acs_data, on=\"GEOID\", how=\"left\")\n\n# Drop columns\nacs_data = acs_data[['GEOID', 'geometry', 'Median_Income', 'Total_Population', 'Total_households', 'Total_Workers',\n       'White_Alone', 'Black_Alone', 'Mean_travel_time', 'No_vehicles', 'One_vehicle', 'Two_vehicles', \n       'Three_vehicles', 'Four_or_more_vehicles', 'Drove_Alone', 'Carpooled',\n       'Public_Transportation', 'Walked', 'Bicycle', 'Worked_at_Home']]\n\nacs_data.head\n\n\n&lt;bound method NDFrame.head of            GEOID                                           geometry  \\\n0    42101001500  POLYGON ((-75.16558 39.94366, -75.16021 39.943...   \n1    42101001800  POLYGON ((-75.16620 39.94081, -75.16599 39.941...   \n2    42101002802  POLYGON ((-75.16735 39.92658, -75.16350 39.929...   \n3    42101004001  POLYGON ((-75.17002 39.92314, -75.16974 39.924...   \n4    42101006300  POLYGON ((-75.24686 39.91876, -75.23967 39.923...   \n..           ...                                                ...   \n403  42101000500  POLYGON ((-75.16506 39.95361, -75.16332 39.953...   \n404  42101027100  POLYGON ((-75.12749 40.05022, -75.12714 40.051...   \n405  42101000200  POLYGON ((-75.16269 39.95623, -75.16234 39.957...   \n406  42101013200  POLYGON ((-75.16096 39.96447, -75.16080 39.965...   \n407  42101032100  POLYGON ((-75.06511 40.01751, -75.06113 40.019...   \n\n    Median_Income Total_Population Total_households Total_Workers White_Alone  \\\n0          108378             3027             1486          1978        2213   \n1          121719             3285             1700          2289        2425   \n2           94427             5868             2432          3148        3525   \n3           82258             4118             2105          2662        3165   \n4           32997             4380             1696          1776          99   \n..            ...              ...              ...           ...         ...   \n403         68977             3292             1480          1591        1351   \n404         60255             2638             1053          1059         157   \n405         97256             3259             1742          1737        1490   \n406         44299             3450             1752          1584        1077   \n407         50409             3686             1629          1766        1466   \n\n    Black_Alone Mean_travel_time No_vehicles One_vehicle Two_vehicles  \\\n0           466            41280         570         758          119   \n1           443            46780         354        1098          165   \n2           319            60580         439        1545          422   \n3           135            70965         465        1112          498   \n4          3667            54680         652         799          133   \n..          ...              ...         ...         ...          ...   \n403        1105            30435        1031         396           53   \n404        1832            38785         165         411          433   \n405         264            31870         452        1108          155   \n406        1563            38195         793         655          252   \n407         757            49210         349        1066          148   \n\n    Three_vehicles Four_or_more_vehicles Drove_Alone Carpooled  \\\n0                5                    34         488       468   \n1               42                    41         716       687   \n2               26                     0        1199      1001   \n3               11                    19        1033       873   \n4               85                    27        1067      1001   \n..             ...                   ...         ...       ...   \n403              0                     0         314       314   \n404             35                     9         699       626   \n405              0                    27         404       283   \n406             52                     0         579       559   \n407             33                    33        1419      1103   \n\n    Public_Transportation Walked Bicycle Worked_at_Home  \n0                     372    377     565            982  \n1                     101    617     690            983  \n2                     618    253     857           1855  \n3                     743    193     483           1291  \n4                     586     84      29            751  \n..                    ...    ...     ...            ...  \n403                   454    444     359            721  \n404                   269      0      80            481  \n405                   249    503     549           1081  \n406                   461    193     328            529  \n407                   221     68      43            842  \n\n[408 rows x 20 columns]&gt;\n\n\n\n\n\nBased on the following maps and bar chart, We know that most workers in Philadelphia commute to work by car. The share of workers who take transit is significantly lower with the exception of North Philadelphia (Strawberry Mansion, Brewerytown). As we will see below, these neighborhoods also have some of the lowest median incomes in Philadelphia. The lack of capital often reduces such users to dependent riders, who will travel by transit regardless of the level of service. However, such riders are also worthy of improved level of service.\n\n\nCode\n# List of columns to plot\ncolumns_to_plot = ['Drove_Alone', 'Carpooled', 'Public_Transportation', 'Walked', 'Bicycle', 'Worked_at_Home']\nacs_data_melted = acs_data.melt(id_vars=['GEOID'], value_vars=columns_to_plot, \n                                var_name='Mode of Transportation', value_name='Count')\n\nbrush = alt.selection_interval()\n\n# Map1: Car use to work\nmap_plot = alt.Chart(acs_data).mark_geoshape().encode(\n    color=alt.Color('Drove_Alone:Q', scale=alt.Scale(scheme='blues')),\n    tooltip=['Drove_Alone:Q', 'Carpooled:Q', 'Public_Transportation:Q', 'Walked:Q', 'Bicycle:Q', 'Worked_at_Home:Q']\n).add_params(brush).properties(\n    width=200,\n    height=400,\n    title=\"Drove Alone to Work\")\n\n# Map2: Public Transportation use to work\nmap_plot1 = alt.Chart(acs_data).mark_geoshape().encode(\n    color=alt.Color('Public_Transportation:Q', scale=alt.Scale(scheme='reds')),\n    tooltip=['Drove_Alone:Q', 'Carpooled:Q', 'Public_Transportation:Q', 'Walked:Q', 'Bicycle:Q', 'Worked_at_Home:Q']\n).add_params(brush).properties(\n    width=200,\n    height=400,\n    title=\"Public Transportation to Work\")\n\n# Map3: Total mode of transportation\nbars = alt.Chart(acs_data_melted).mark_bar().encode(\n    x=alt.X('sum(Count):Q', title=\"Total Number of Workers by Mode of Transportation\"),\n    y=alt.Y('Mode of Transportation:N', title=\"Mode of Transportation\"),\n    color='Mode of Transportation:N'\n).transform_filter(\n    brush\n).properties(width=300)\n\n# Combine the maps\nchart = alt.vconcat(\n    alt.hconcat(map_plot, map_plot1), bars)\n\nchart\n\n\n\n\n\n\n\n\n\n\n\nFor our analysis, we aimed to examine key indicators that could influence a commuter’s mode choice. We first compared the total population and total number of workers in each census tract to identify where work-based trips are most likely to be generated. The two graphs show a strong correlation, indicating that areas with larger populations also tend to have a higher number of workers.\nIn 2023, the median household income in Philadelphia was $60,698, adjusted for inflation. However, when visualizing the data on the map, income disparities become apparent. High-income areas are concentrated in the Northwest and South, while the West, North, and Northeast regions fall below the median income threshold, highlighting areas of economic inequality.\nWe explored travel time across different census tracts to identify areas with greater mobility challenges. As expected, regions farther from Center City experience longer travel times. However, it is noteworthy that travel times for residents in North Philly and Lower West Philly are higher than those in other nearby areas, despite their proximity to the city center.\nOur map also reveals several key correlations. Areas with a majority of Black households tend to have a higher proportion of households without vehicles. Additionally, these areas generally exhibit lower median incomes, further indicating socioeconomic disparities that influence transportation needs and mode choice across the city.\n\n\nCode\n# Convert object columns to numeric, coercing errors into NaN (missing data)\ncols_to_convert = ['Median_Income', 'Total_Population', 'Total_Workers', 'White_Alone', 'Black_Alone',\n                   'Mean_travel_time', 'No_vehicles', 'One_vehicle', 'Two_vehicles', \n                   'Three_vehicles', 'Four_or_more_vehicles']\n\nfor col in cols_to_convert:\n    acs_data[col] = pd.to_numeric(acs_data[col], errors='coerce')\n    \n# Calculate the share of each race relative to the total population\nacs_data['White_Share'] = acs_data['White_Alone'] / acs_data['Total_Population']\nacs_data['Black_Share'] = acs_data['Black_Alone'] / acs_data['Total_Population']\n\n# Assign the race with the highest share\nacs_data['race_majority'] = acs_data[['White_Share', 'Black_Share']].idxmax(axis=1)\n\n# Clean data\nacs_data = acs_data[acs_data['Median_Income'] != -666666666]\n\n# Calculate Mean travel time in minutes\nacs_data['Average_travel_time'] = (acs_data['Mean_travel_time'] / acs_data['Total_Workers']).fillna(0).apply(lambda x: max(x, 0))\nacs_data['Average_travel_time'] = acs_data['Average_travel_time'].round(0).astype(int)\n\n\nC:\\Users\\USER\\miniforge3\\envs\\musa-550-fall-2023\\lib\\site-packages\\geopandas\\geodataframe.py:1538: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\nCode\n# Create a list of the columns to plot\ncolumns_to_plot = ['Total_Population', 'Total_Workers', 'Median_Income','Average_travel_time', 'No_vehicles', 'race_majority']\n\n# Set up a grid of subplots\nfig, axes = plt.subplots(3, 2, figsize=(8, 12)) \naxes = axes.flatten() \n\n# Loop through the columns to plot each one\nfor i, column in enumerate(columns_to_plot):\n    ax = axes[i]\n    acs_data.plot(column=column, ax=ax, legend=True, cmap='Blues')\n    ax.set_title(f\"{column}\")\n    ax.set_axis_off()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWhile researching a transit rider vulnerability matrix, we consistently found that exposure to heat was a critical factor. However, vulnerability to heat is closely correlated with several other variables, many of which we have already explored. As a result, we decided to incorporate the City’s Heat Vulnerability Index (HVI) to highlight areas most at risk of experiencing heat-related illness.\nThe Philadelphia Heat Vulnerability Index (HVI) captures key factors that contribute to the negative health effects of extreme heat events. The HVI rating is calculated by combining both exposure and sensitivity indicators:\n\nAge: The percentage of the population over 65 years old, as older individuals are more susceptible to heat-related illnesses.\nEducational Attainment: The percentage of individuals over 25 years old without a high school diploma, which can be linked to lower awareness of heat risks.\nLanguage Barrier: The percentage of limited English-speaking households, which can affect the ability to receive heat-related warnings and information.\nLow Socioeconomic Status: The percentage of the population living below the Federal Poverty Level, as economically disadvantaged individuals may have fewer resources to adapt to extreme heat.\nRace & Ethnicity: The percentage of the population identifying as non-white, as certain racial and ethnic groups may experience heightened vulnerability due to social and economic factors.\nSocial Isolation: The percentage of the population over 65 years old living alone, as socially isolated individuals may lack support during heat events.\nHealth Status: This factor considers the underlying health conditions that may exacerbate the effects of extreme heat, such as chronic diseases or disabilities that limit mobility or access to cooling resources.\n\n\n\nCode\nheat = pd.read_csv('data\\heat_vulnerability_ct.csv')\nheat.rename(columns={'geoid10': 'GEOID'}, inplace=True)\n\n# Merge the heat vulnerability data with acs data\nheat['GEOID'] = heat['GEOID'].astype(str)\nacs_data_index = acs_data.merge(heat, on='GEOID')\n\n# List of the columns to plot\ncolumns_to_plot = ['hsi_score','hei_score', 'hvi_score', 'n_veryhigh']\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 6))\naxes = axes.flatten() \n\n# Loop through the columns and plot each one\nfor i, column in enumerate(columns_to_plot):\n    ax = axes[i]\n    acs_data_index.plot(column=column, ax=ax, legend=True, cmap='Blues')\n    ax.set_title(f\"{column}\")\n    ax.set_axis_off()  \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe City’s Heat Vulnerability Index (HVI) incorporates several socio-demographic parameters to assess vulnerability, and for our Transit Vulnerability Index (TVI), we have integrated these factors with transit-specific parameters, such as travel time and lack of available household vehicles. However, by focusing solely on these three factors, we may overlook households that, although located far from transit routes and reliant on personal vehicles for commuting, still experience economic disadvantage. To address this gap, we reintroduced the income variable to better account for such populations.\nGiven that this assessment is focused primarily on transit, we assigned a value of 1 to both median income and HVI, and a value of 2 to commute time and access to vehicles, reflecting their relative importance in influencing transit vulnerability. Our final TVI, ranging from 0 to 6, provides a comprehensive vulnerability score for census tracts across Philadelphia. Notably, the map below highlights North Philadelphia, particularly Strawberry Mansion, and West Philadelphia, specifically Cobbs Creek, as areas where transit riders are particularly vulnerable.\n\n\nCode\n# Social Vulnerability Index (SVI)\n\nhighest_quartile_travel_time = acs_data_index['Average_travel_time'].quantile(0.75)\nhighest_quartile_no_vehicles = acs_data_index['No_vehicles'].quantile(0.75)\nlowest_quartile_median_income = acs_data_index['Median_Income'].quantile(0.25)\n\nacs_data_index['Social_Vulnerability_Index'] = 0\n\n# Assign points\nacs_data_index.loc[acs_data_index['n_veryhigh'] == 1, 'Social_Vulnerability_Index'] += 1\nacs_data_index.loc[acs_data_index['Median_Income'] &lt;= lowest_quartile_median_income, 'Social_Vulnerability_Index'] += 1\nacs_data_index.loc[acs_data_index['Average_travel_time'] &gt;= highest_quartile_travel_time, 'Social_Vulnerability_Index'] += 2\nacs_data_index.loc[acs_data_index['No_vehicles'] &gt;= highest_quartile_no_vehicles, 'Social_Vulnerability_Index'] += 2\nacs_data_index[['n_veryhigh', 'Average_travel_time', 'No_vehicles', 'Median_Income','Social_Vulnerability_Index']].head()\n\n# Create the first map plot (Choropleth map)\nmap_plot = alt.Chart(acs_data_index).mark_geoshape().encode(\n    color=alt.Color('Social_Vulnerability_Index:Q', scale=alt.Scale(scheme='blues')),\n    tooltip=['GEOID:N', 'n_veryhigh:Q', 'Average_travel_time:Q', 'No_vehicles:Q', 'Median_Income:Q','race_majority:N']\n).add_params(brush).properties(\n    width=500,\n    height=500)\n\nchart = alt.vconcat(map_plot)\nchart"
  },
  {
    "objectID": "GTFS.html",
    "href": "GTFS.html",
    "title": "Mapping Transit Network",
    "section": "",
    "text": "#Load Packages:\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport osmnx as ox\nimport shapely\nfrom shapely.geometry import Point\nimport folium\nfrom folium import Choropleth"
  },
  {
    "objectID": "GTFS.html#create-philadelphias-bus-network",
    "href": "GTFS.html#create-philadelphias-bus-network",
    "title": "Mapping Transit Network",
    "section": "Create Philadelphia’s Bus Network:",
    "text": "Create Philadelphia’s Bus Network:\nConvert all shapes from points to linestrings:\n\nprint(shapes_df)\n\n        shape_id  shape_pt_lat  shape_pt_lon  shape_pt_sequence\n0         286406     39.976568    -75.196097                206\n1         286406     40.006143    -75.196839                417\n2         286406     40.006429    -75.197471                420\n3         286410     39.985246    -75.208209                294\n4         286410     39.986857    -75.207831                307\n...          ...           ...           ...                ...\n536088    286742     39.945364    -75.326472                440\n536089    286753     39.918701    -75.264710                178\n536090    286659     39.949104    -75.275955                152\n536091    286660     39.960680    -75.349991                142\n536092    286661     39.950913    -75.267558                125\n\n[536093 rows x 4 columns]\n\n\n\nfrom shapely.geometry import LineString\n\n# Ensure the DataFrame is sorted by shape_id and shape_pt_sequence\nshapes_df = shapes_df.sort_values(by=['shape_id', 'shape_pt_sequence'])\n\n# Group by shape_id and create LineString for each group\nlines = (\n    shapes_df.groupby('shape_id')\n    .apply(lambda group: LineString(zip(group['shape_pt_lon'], group['shape_pt_lat'])))\n    .reset_index(name='geometry')\n)\n\n# Create a GeoDataFrame from the lines\nbus_network = gpd.GeoDataFrame(lines, geometry='geometry', crs='EPSG:4326')\n\nbus_network.plot()\n\n\n\n\n\n\n\n\n\nTrim Bus Network to Philadelphia City Boundary\nAdd city boundary:\n\ncity_boundary = ox.geocode_to_gdf(\"Philadelphia, Pennsylvania, USA\")\n\ncity_boundary.plot()\n\n\n\n\n\n\n\n\nTrim bus network:\n\n# Clip the shapes to the Philadelphia boundary\nbus_network_philadelphia = gpd.clip(bus_network, city_boundary)\n\nbus_network_philadelphia.plot()\n\n\n\n\n\n\n\n\nAt this step, we ensure that our segment data is working as desired so that it can combine with our points data.\n\nfrom shapely.geometry import Point, LineString, MultiLineString\n\n# Function to convert MultiLineString to LineString\ndef convert_to_linestring(geometry):\n    if isinstance(geometry, MultiLineString):\n        # Combine all components into a single LineString\n        combined_coords = []\n        for line in geometry.geoms:  # Use .geoms to iterate over the components\n            combined_coords.extend(line.coords)\n        return LineString(combined_coords)\n    return geometry  # If already a LineString, return as is\n\n# Apply the conversion function to the GeoDataFrame\nbus_network_philadelphia['geometry'] = bus_network_philadelphia['geometry'].apply(convert_to_linestring)\n\n\n\nimport networkx as nx\nfrom shapely.ops import split, unary_union\n\n# Create a graph from the GeoDataFrame\ndef geodataframe_to_graph(gdf):\n    G = nx.Graph()\n    for line in gdf.geometry:\n        if isinstance(line, LineString):\n            coords = list(line.coords)\n            for i in range(len(coords) - 1):\n                G.add_edge(Point(coords[i]), Point(coords[i + 1]), geometry=LineString(coords[i:i + 2]))\n    return G\n\n# Convert GeoDataFrame to a graph\ngraph = geodataframe_to_graph(bus_network_philadelphia)\n\n# Split the network into edges divided by nodes\nedges = []\nfor edge in graph.edges(data=True):\n    line = edge[2]['geometry']\n    start_node = edge[0]\n    end_node = edge[1]\n    edges.append({\n        'geometry': line,\n        'start': start_node,\n        'end': end_node\n    })\n\n# Create a GeoDataFrame of edges\nedges_gdf = gpd.GeoDataFrame(edges, crs=bus_network_philadelphia.crs)\n\nedges_gdf = gpd.clip(edges_gdf, city_boundary)\n\nedges_gdf.plot()\n\n\n\n\n\n\n\n\n\n\nTotal daily buses arriving at each bus stop:\nBy aggregating this data, we estimated the daily arrivals at each bus stop, effectively quantifying the daily traffic across the network. The distribution, as illustrated in the bar plot below, reveals that while a small number of stops experience exceptionally high traffic exceeding 2,000 buses per day (likely transit interchange hubs), the majority of bus stops accommodate approximately 183 buses daily.\n\ndaily_buses = stop_times_df.groupby([\"stop_id\"]).size().reset_index(name=\"daily_arrivals\")\n\ndaily_buses_sorted = daily_buses.sort_values(by=\"daily_arrivals\", ascending=False).reset_index(drop=True)\n\ndaily_buses_sorted.head()\n\n\n\n\n\n\n\n\nstop_id\ndaily_arrivals\n\n\n\n\n0\n283\n3876\n\n\n1\n593\n2592\n\n\n2\n21204\n2323\n\n\n3\n10266\n2252\n\n\n4\n1148\n2219\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\n\n# Create the histogram with bins in intervals of 400\nplt.hist(\n    daily_buses[\"daily_arrivals\"],\n    bins=range(0, daily_buses[\"daily_arrivals\"].max() + 100, 100),  # Adjust bins\n    edgecolor=\"black\",\n    align=\"left\"\n)\n\n# Set titles and labels\nplt.title(\"Distribution of Bus Stops by Total Daily Bus Arrivals\", fontsize=16)\nplt.xlabel(\"Number of Daily Bus Arrivals\", fontsize=12)\nplt.ylabel(\"Number of Bus Stops\", fontsize=12)\n\n# Set x-ticks to match the bin intervals\nplt.xticks(range(0, daily_buses[\"daily_arrivals\"].max() + 400, 400))\n\n# Add a grid for better readability\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTotal Number and Peak Period\nThe data was subsequently grouped by each hour of the day to analyze the temporal distribution of bus service. The bar chart below illustrates the hourly distribution, revealing two distinct periods of heightened bus frequency: between 7 AM and 8 AM, and from 3 PM to 6 PM, with consistently elevated service levels during the midday hours. Bus frequency gradually declines after 8 PM until 5 AM. For the purpose of this analysis, the 3 PM to 6 PM window is designated as the peak period due to its sustained high frequency and significance in transit operations.\n\nstop_times_df[\"hour\"] = stop_times_df[\"arrival_time\"].str.slice(0, 2).astype(int)\n\n# Group by trip_id and hour, and count rows\nhourly_bus_arrivals = stop_times_df.groupby([\"stop_id\", \"hour\"]).size().reset_index(name=\"hourly_arrivals\")\n\nhourly_bus_arrivals_sorted = hourly_bus_arrivals.sort_values(by=\"hourly_arrivals\", ascending=False).reset_index(drop=True)\n\nhourly_bus_arrivals_sorted.head()\n\n\n\n\n\n\n\n\nstop_id\nhour\nhourly_arrivals\n\n\n\n\n0\n283\n16\n287\n\n\n1\n283\n17\n278\n\n\n2\n283\n8\n273\n\n\n3\n283\n15\n266\n\n\n4\n283\n7\n232\n\n\n\n\n\n\n\n\n\nFind number of buses per hour on the entire system to determine peak period:\n\ntotal_hourly_buses = stop_times_df.groupby([\"hour\"]).size().reset_index(name=\"total_bus_arrivals\")\n\ntotal_hourly_buses = total_hourly_buses[total_hourly_buses[\"hour\"] &lt;= 24]\n\ntotal_hourly_buses = total_hourly_buses.sort_values(by=\"total_bus_arrivals\", ascending=False).reset_index(drop=True)\n\ntotal_hourly_buses.head()\n\n\n\n\n\n\n\n\nhour\ntotal_bus_arrivals\n\n\n\n\n0\n16\n188679\n\n\n1\n15\n187335\n\n\n2\n17\n184023\n\n\n3\n7\n177282\n\n\n4\n8\n176510\n\n\n\n\n\n\n\n\n\nFind Peak Period\n\nplt.figure(figsize=(10, 6))\nplt.bar(total_hourly_buses[\"hour\"], total_hourly_buses[\"total_bus_arrivals\"], edgecolor=\"black\")\n\n# Add labels and title\nplt.title(\"Total Bus Arrivals by Hour\", fontsize=16)\nplt.xlabel(\"Hour\", fontsize=12)\nplt.ylabel(\"Total Daily Bus Arrivals\", fontsize=12)\nplt.xticks(total_hourly_buses[\"hour\"])  # Ensure x-axis ticks align with hours\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBus Frequency\nIn transit planning, frequency refers to the number of buses servicing a particular stop within a given time frame, typically measured as buses per hour. It is a critical metric for understanding the level of service provided by a transit network, as higher frequency generally translates to shorter wait times for passengers and improved overall service reliability. In this analysis, frequency is calculated by determining the number of bus arrivals at each stop during specified time periods (3 PM to 6 PM).\n\npeak_bus_arrivals = hourly_bus_arrivals[hourly_bus_arrivals[\"hour\"].isin([3, 4, 5])]\n\npeak_frequency = peak_bus_arrivals.groupby(\"stop_id\", as_index=False)[\"hourly_arrivals\"].sum()\n\npeak_frequency = peak_frequency.rename(columns={\"hourly_arrivals\": \"bus_arrivals\"})\n\n# Create a new column 'frequency'\npeak_frequency[\"frequency\"] = peak_frequency[\"bus_arrivals\"] / 3\n\npeak_frequency.head()\n\n\n\n\n\n\n\n\nstop_id\nbus_arrivals\nfrequency\n\n\n\n\n0\n2\n23\n7.666667\n\n\n1\n4\n46\n15.333333\n\n\n2\n5\n37\n12.333333\n\n\n3\n7\n6\n2.000000\n\n\n4\n8\n66\n22.000000\n\n\n\n\n\n\n\nNote: Frequency is the number of buses per hour at a bus stop\n\n\nBus Headways\nAdditionally, headways—the time interval between consecutive buses at a stop—are directly derived from frequency. It directly impacts passenger experience, as shorter headways result in reduced waiting times and greater convenience, particularly during peak periods when demand is highest. Here, we first calculate the number of bus arrivals at a stop during peak period. After getting the per hour frequency, we calculate the headway at a bus stop by dividing 60 minutes. On an average the headway is 30 minutes.\n\n# Calculate the headway and assign it to a new column\npeak_frequency[\"headway\"] = 60 / peak_frequency[\"frequency\"]\n\npeak_frequency = peak_frequency.sort_values(by=\"headway\", ascending=True).reset_index(drop=True)\n\n# Display the updated DataFrame\nprint(peak_frequency)\n\n      stop_id  bus_arrivals  frequency     headway\n0       10266           231  77.000000    0.779221\n1         283           194  64.666667    0.927835\n2        1148           176  58.666667    1.022727\n3       31564           166  55.333333    1.084337\n4         341           111  37.000000    1.621622\n...       ...           ...        ...         ...\n9981    27732             1   0.333333  180.000000\n9982    22380             1   0.333333  180.000000\n9983    22370             1   0.333333  180.000000\n9984    22388             1   0.333333  180.000000\n9985    18183             1   0.333333  180.000000\n\n[9986 rows x 4 columns]\n\n\nNote: Headway refers to number of minutes in between bus arrivals\n\n\nMerge calculated bus metrics with the stops dataframe containing stop names and co-ordinates:\n\npeak_metrics = pd.merge(peak_frequency, stops_df, on=\"stop_id\", how=\"inner\")\n\npeak_metrics.head()\n\n\n\n\n\n\n\n\nstop_id\nbus_arrivals\nfrequency\nheadway\nstop_name\nstop_lat\nstop_lon\nlocation_type\nparent_station\nzone_id\nwheelchair_boarding\n\n\n\n\n0\n10266\n231\n77.000000\n0.779221\nMarket St & 15th St\n39.952547\n-75.165475\nNaN\nNaN\n1\n1\n\n\n1\n283\n194\n64.666667\n0.927835\n13th St\n39.952532\n-75.162559\nNaN\nNaN\n1\n2\n\n\n2\n1148\n176\n58.666667\n1.022727\n69th St Transportation Center South Terminal\n39.962079\n-75.258284\nNaN\n31034.0\n1\n1\n\n\n3\n31564\n166\n55.333333\n1.084337\n15th St & Market St - FS\n39.952493\n-75.165393\nNaN\nNaN\n1\n1\n\n\n4\n341\n111\n37.000000\n1.621622\nRichmond St & Westmoreland St Loop\n39.984253\n-75.099553\nNaN\nNaN\n1\n1\n\n\n\n\n\n\n\n\n\nBus Ridership:\nWe then merge the ridership at each stop to our existing dataset containing frequency and headway.\n\nridership = pd.read_csv(\"Data/Fall_2023_Stop_Summary_(Bus).csv\")\n\nridership[\"Ridership\"] = ridership[\"Weekday_On\"] + ridership[\"Weekday_Of\"]\n\nridership.rename(columns={\"Stop_Code\": \"stop_id\"}, inplace=True)\n\nridership_by_stop = ridership.groupby(\"stop_id\")[\"Ridership\"].sum().reset_index()\n\nridership_by_stop.head()\n\n\n\n\n\n\n\n\nstop_id\nRidership\n\n\n\n\n0\n2\n1188\n\n\n1\n4\n666\n\n\n2\n5\n448\n\n\n3\n7\n41\n\n\n4\n8\n1280\n\n\n\n\n\n\n\n\n#Merge Ridership and Schedules\n\npeak_metrics = pd.merge(peak_metrics, ridership_by_stop, on=\"stop_id\", how=\"inner\")\n\nprint(peak_metrics)\n\n      stop_id  bus_arrivals  frequency     headway  \\\n0       10266           231  77.000000    0.779221   \n1        1148           176  58.666667    1.022727   \n2       31564           166  55.333333    1.084337   \n3         341           111  37.000000    1.621622   \n4         841           103  34.333333    1.747573   \n...       ...           ...        ...         ...   \n9140    27732             1   0.333333  180.000000   \n9141    22380             1   0.333333  180.000000   \n9142    22370             1   0.333333  180.000000   \n9143    22388             1   0.333333  180.000000   \n9144    18183             1   0.333333  180.000000   \n\n                                         stop_name   stop_lat   stop_lon  \\\n0                              Market St & 15th St  39.952547 -75.165475   \n1     69th St Transportation Center South Terminal  39.962079 -75.258284   \n2                         15th St & Market St - FS  39.952493 -75.165393   \n3               Richmond St & Westmoreland St Loop  39.984253 -75.099553   \n4                  Fern Rock Transportation Center  40.041940 -75.136970   \n...                                            ...        ...        ...   \n9140                State Rd & Pennypack St - MBFS  40.043558 -74.998562   \n9141                Bustleton Av & Scotchbrook Dr   40.082637 -75.040538   \n9142                        Welsh Rd & Michael Rd   40.063238 -75.028130   \n9143                      Alburger Av & Verree Rd   40.094735 -75.050601   \n9144                    Washington Ln & Chew -MBNS  40.051825 -75.172181   \n\n      location_type  parent_station  zone_id  wheelchair_boarding  Ridership  \n0               NaN             NaN        1                    1       2848  \n1               NaN         31034.0        1                    1       4758  \n2               NaN             NaN        1                    1        539  \n3               NaN             NaN        1                    1        222  \n4               NaN             NaN        1                    1       1509  \n...             ...             ...      ...                  ...        ...  \n9140            NaN             NaN        1                    1          0  \n9141            NaN             NaN        1                    1          2  \n9142            NaN             NaN        1                    1         14  \n9143            NaN             NaN        1                    1         16  \n9144            NaN             NaN        1                    1         12  \n\n[9145 rows x 12 columns]\n\n\nOur analysis aimed to determine whether higher frequency (or shorter headways) correlates with greater ridership. While ridership and frequency exhibit a positive correlation, the presence of many outliers highlights the need for a combined analysis of both variables. This is crucial for identifying streets that would benefit most from improvements such as bus lanes, and it suggests that increasing frequency may be necessary to better accommodate the current ridership levels.\n\nimport numpy as np\n\n# Ensure data is numeric and drop rows with NaN values for the columns of interest\npeak_metrics_cleaned = peak_metrics.dropna(subset=['frequency', 'Ridership'])\n\n# Extract the data\nx = peak_metrics_cleaned['frequency']\ny = peak_metrics_cleaned['Ridership']\n\n# Fit a linear regression model\ncoefficients = np.polyfit(x, y, 1)  # Degree 1 for linear regression\ntrendline = np.poly1d(coefficients)\n\n# Create the scatterplot\nplt.figure(figsize=(8, 6))\nplt.scatter(x, y, alpha=0.7, label='Data')\n\n# Add the trend line\nplt.plot(x, trendline(x), color='red', linestyle='--', label='Trend Line')\n\n# Add titles and labels\nplt.title('Scatterplot of Frequency vs Ridership with Trend Line', fontsize=16)\nplt.xlabel('Frequency', fontsize=12)\nplt.ylabel('Ridership', fontsize=12)\n\n# Add legend\nplt.legend()\n\n# Show grid\nplt.grid(True, linestyle='--', alpha=0.5)\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\nThis presumption is supported by the scatter-plot below, which reveals that stops with extremely high ridership are concentrated near the origin, corresponding to headways of under 25 minutes. We can also see that areas with greater headways have their ridership closer to origin well- suggesting lower ridership demand.\n\n# Scatterplot: Ridership vs. Headway\nplt.figure(figsize=(8, 6))\n\nplt.scatter(\n    peak_metrics[\"headway\"],\n    peak_metrics[\"Ridership\"],\n    c=\"blue\",\n    alpha=0.4,)\n\nplt.title(\"Scatterplot: Ridership vs. Headway\", fontsize=16)\nplt.xlabel(\"Headway (minutes)\", fontsize=12)\nplt.ylabel(\"Ridership\", fontsize=12)\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\nConvert Bus Data to GeoDataframe\n\ngeometry = [Point(xy) for xy in zip(peak_metrics[\"stop_lon\"], peak_metrics[\"stop_lat\"])]\npeak_metrics_geo = gpd.GeoDataFrame(peak_metrics, geometry=geometry)\n\n# Set a Coordinate Reference System (CRS)\npeak_metrics_geo.set_crs(epsg=4326, inplace=True)  # WGS84 CRS\n\npeak_metrics_geo.head()\n\n\n\n\n\n\n\n\nstop_id\nbus_arrivals\nfrequency\nheadway\nstop_name\nstop_lat\nstop_lon\nlocation_type\nparent_station\nzone_id\nwheelchair_boarding\nRidership\ngeometry\n\n\n\n\n0\n10266\n231\n77.000000\n0.779221\nMarket St & 15th St\n39.952547\n-75.165475\nNaN\nNaN\n1\n1\n2848\nPOINT (-75.16548 39.95255)\n\n\n1\n1148\n176\n58.666667\n1.022727\n69th St Transportation Center South Terminal\n39.962079\n-75.258284\nNaN\n31034.0\n1\n1\n4758\nPOINT (-75.25828 39.96208)\n\n\n2\n31564\n166\n55.333333\n1.084337\n15th St & Market St - FS\n39.952493\n-75.165393\nNaN\nNaN\n1\n1\n539\nPOINT (-75.16539 39.95249)\n\n\n3\n341\n111\n37.000000\n1.621622\nRichmond St & Westmoreland St Loop\n39.984253\n-75.099553\nNaN\nNaN\n1\n1\n222\nPOINT (-75.09955 39.98425)\n\n\n4\n841\n103\n34.333333\n1.747573\nFern Rock Transportation Center\n40.041940\n-75.136970\nNaN\nNaN\n1\n1\n1509\nPOINT (-75.13697 40.04194)\n\n\n\n\n\n\n\n\npeak_metrics_geo.plot()\n\n\n\n\n\n\n\n\nTo focus on bus stops within the city limits, we restrict the data to those located inside the city boundary:\n\npeak_metrics_geo = gpd.clip(peak_metrics_geo, city_boundary)\n\n\npeak_metrics_geo.plot()"
  },
  {
    "objectID": "GTFS.html#level-of-service-analysis",
    "href": "GTFS.html#level-of-service-analysis",
    "title": "Mapping Transit Network",
    "section": "Level of Service Analysis",
    "text": "Level of Service Analysis\nTo better understand the spatial distribution of bus network, we first clipped the network to city limits."
  },
  {
    "objectID": "GTFS.html#visualise-metrics-by-bus-stop",
    "href": "GTFS.html#visualise-metrics-by-bus-stop",
    "title": "Mapping Transit Network",
    "section": "Visualise metrics by bus stop:",
    "text": "Visualise metrics by bus stop:\nTo evaluate the level of service across Philadelphia and understand user patterns, we visualized three key components—frequency, headway, and ridership—on a map for peak periods. The map reveals that most areas in the city benefit from frequent bus services, with frequency under 10 during peak times and averaging below 20 minutes headway across the day. However, there are extreme outliers in some regions, where headway exceed 180 minutes, highlighting disparities in service provision. By adding a geographic component, we observe a more nuanced distribution of ridership across Philadelphia. As expected, areas in Center City exhibit high ridership and frequent bus service. Conversely, far-flung areas such as Upper Northwest and Lower Northwest show lower ridership levels and greater headway. Interestingly, Lower Northeast stands out with the highest ridership of all districts.\nInterestingly, while frequency and headway vary significantly across the city, ridership appears more evenly distributed, with notable exceptions in Center City, which experiences concentrated high ridership, and the Lower Northwest, near Mt. Airy, which shows higher ridership despite longer headway.\n\nimport ipywidgets as widgets\n\n# create function\ndef create_map(selected_metric):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    peak_metrics_geo.plot(\n        ax=ax,\n        column=selected_metric, \n        cmap=\"plasma\",            \n        legend=True,              \n        markersize=2)\n    \n    \n    plt.title(f\"{selected_metric.capitalize()} by Bus Stop\", fontsize=16)\n    plt.xlabel(\"Longitude\")\n    plt.ylabel(\"Latitude\")\n    plt.show()\n\ndropdown = widgets.Dropdown(\n    options=['frequency', 'headway', 'Ridership'],  \n    value='frequency',\n    description='Metric:',)\n\nwidgets.interactive(create_map, selected_metric=dropdown)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nFile ~\\miniforge3\\envs\\musa-550-fall-2023\\lib\\site-packages\\ipywidgets\\widgets\\interaction.py:243, in interactive.update(self, *args)\n    241     value = widget.get_interact_value()\n    242     self.kwargs[widget._kwarg] = value\n--&gt; 243 self.result = self.f(**self.kwargs)\n    244 show_inline_matplotlib_plots()\n    245 if self.auto_display and self.result is not None:\n\nCell In[1], line 5, in create_map(selected_metric)\n      4 def create_map(selected_metric):\n----&gt; 5     fig, ax = plt.subplots(figsize=(8, 8))\n      6     peak_metrics_geo.plot(\n      7         ax=ax,\n      8         column=selected_metric, \n      9         cmap=\"plasma\",            \n     10         legend=True,              \n     11         markersize=2)\n     14     plt.title(f\"{selected_metric.capitalize()} by Bus Stop\", fontsize=16)\n\nNameError: name 'plt' is not defined"
  },
  {
    "objectID": "GTFS.html#visualisation-of-metrics-by-street-segments-on-philadelphias-bus-network",
    "href": "GTFS.html#visualisation-of-metrics-by-street-segments-on-philadelphias-bus-network",
    "title": "Mapping Transit Network",
    "section": "Visualisation of Metrics by Street Segments on Philadelphia’s Bus Network",
    "text": "Visualisation of Metrics by Street Segments on Philadelphia’s Bus Network\nFollowing the spatial join, we can now analyze bus headways during peak hours by street segments, providing a more refined view of service levels across different parts of the city. Major corridors in Philadelphia, such as Broad Street, Walnut Street, Chestnut Street, Market Street, and Roosevelt Boulevard, exhibit headways of under 20 minutes, reflecting frequent service. In contrast, streets on the city’s periphery have significantly longer headways, exceeding 120 minutes, highlighting areas with less frequent service.\n\nfig, ax = plt.subplots(figsize=(10, 8))\npeak_metrics_network.plot(\n    ax=ax,\n    column=\"headway\",  # Color by buses_count\n    cmap=\"plasma\",        # Use a colormap (e.g., viridis, plasma, etc.)\n    legend=True,           # Add a legend\n    markersize=1         # Adjust marker size\n)\n\n# Add titles and labels\nplt.title(\"Headway by Street Segment\", fontsize=16)\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\n\nText(110.89550110614813, 0.5, 'Latitude')\n\n\n\n\n\n\n\n\n\nWhen examining ridership by street segments, the distribution tells a different story. Ridership remains relatively consistent across most of the network, regardless of the headway. However, ridership spikes above 2,000 passengers for specific routes around Mt. Airy, Frankford, N Broad St., as well as Center City. These areas, particularly in the North, host major transportation hubs that attract a high volume of trips.\n\nfig, ax = plt.subplots(figsize=(10, 8))\npeak_metrics_network.plot(\n    ax=ax,\n    column=\"Ridership\",  # Color by buses_count\n    cmap=\"plasma\",        # Use a colormap (e.g., viridis, plasma, etc.)\n    legend=True,           # Add a legend\n    markersize=1         # Adjust marker size\n)\n\n# Add titles and labels\nplt.title(\"Ridership by Street Segment\", fontsize=16)\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\n\nText(110.89550110614813, 0.5, 'Latitude')\n\n\n\n\n\n\n\n\n\n\npeak_metrics_network = peak_metrics_network.drop(columns=['start', 'end'], errors='ignore')  # Replace 'start', 'end' with problematic columns\n\npeak_metrics_network.to_file(\"Data/peak_metrics_network.geojson\", driver=\"GeoJSON\")"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "This research utilizes a combination of ridership data, bus operational performance metrics, and demographic datasets to identify key areas for impactful transit solutions in Philadelphia.\nSEPTA GTFS API The General Transit Feed Specification (GTFS) API provides standardized transit data, including schedules, routes, stops, and fare information for SEPTA buses, enabling detailed analysis of transit operations.\nOSM Street Network: OpenStreetMap (OSM) data accessed through the osmnx Python package enables the extraction of street networks in Philadelphia.\nSEPTA Ridership Data: SEPTA’s ridership statistics provide insights into the number of passengers using different bus routes, helping identify high-demand corridors.\nHeat Vulnerability Data: The Heat Vulnerability dataset highlights areas in Philadelphia most at risk during extreme heat events. It integrates social and environmental factors to pinpoint vulnerable populations.\nCensus API/ACS: The American Community Survey (ACS) API provides demographic data at various geographic levels to help analyze population characteristics such as income, age, and vehicle ownership.\nDVRPC/AADT: The Delaware Valley Regional Planning Commission (DVRPC) provides Annual Average Daily Traffic (AADT) data, which includes traffic count locations across the Philadelphia region.\nPhiladelphia Vision Zero High Injury Network: This dataset outlines the high-injury network in Philadelphia, identifying locations with the highest frequency of traffic-related injuries and fatalities."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 550: Final Project- Riya & Varun",
    "section": "",
    "text": "The project, in alignment with SEPTA’s Bus Revolution network redesign, aims to identify key streets and intersections in Philadelphia that serve the most buses and bus passengers as well as identify areas with the most vulnerable populations in terms of mobility to propose locations for impactful transit solutions such as bus lanes and transit signal prioritization. By leveraging multiple datasets, the project seeks to improve bus service efficiency and equity in public transit. \nWe specifically look at how a data-driven approach, leveraging ridership patterns, bus operational performance, and demographic indicators, can be utilized to recommend strategic interventions that not only improve transit service reliability but also address mobility challenges for underserved communities.\n\n\n\n\n\n\n\nSEPTA Bus Revolution Graphic"
  },
  {
    "objectID": "index.html#research-question",
    "href": "index.html#research-question",
    "title": "MUSA 550: Final Project- Riya & Varun",
    "section": "",
    "text": "The project, in alignment with SEPTA’s Bus Revolution network redesign, aims to identify key streets and intersections in Philadelphia that serve the most buses and bus passengers as well as identify areas with the most vulnerable populations in terms of mobility to propose locations for impactful transit solutions such as bus lanes and transit signal prioritization. By leveraging multiple datasets, the project seeks to improve bus service efficiency and equity in public transit. \nWe specifically look at how a data-driven approach, leveraging ridership patterns, bus operational performance, and demographic indicators, can be utilized to recommend strategic interventions that not only improve transit service reliability but also address mobility challenges for underserved communities.\n\n\n\n\n\n\n\nSEPTA Bus Revolution Graphic"
  },
  {
    "objectID": "index.html#methodology",
    "href": "index.html#methodology",
    "title": "MUSA 550: Final Project- Riya & Varun",
    "section": "Methodology",
    "text": "Methodology\nTo identify opportunities for bus lanes and transit signal prioritization, our analysis focuses on three key objectives:\nTransit Mapping: Understanding the Existing Bus Network. We analyze the current bus network to identify routes with the highest frequency and ridership, providing a baseline for prioritizing infrastructure improvements. It ensures that proposed interventions will have the greatest operational impact and benefit the most riders.\nTransit Vulnerability: Addressing Social Disparities. We Recognize that focusing solely on infrastructure and ridership data may overlook critical social inequities. Therefore, we developed a Transit Vulnerability Index that highlights populations most in need of transit reforms, incorporating key demographic and accessibility criteria.\nTransit Priority: Integrating Social and Network Data. By combining insights from the Transit Vulnerability Index with bus network performance, we identify and prioritize 10 areas or bus routes that demonstrate the greatest need for interventions."
  },
  {
    "objectID": "final.html",
    "href": "final.html",
    "title": "Authors",
    "section": "",
    "text": "Scope for Further Research\nIn the future, this analytical approach can serve as a powerful tool for evaluating whether a city’s transit system effectively serves its most vulnerable populations. By combining network performance data with social vulnerability insights, we can pinpoint areas in need of targeted interventions, such as the introduction of bus lanes or transit signal prioritization. This methodology, initially applied to cities like Philadelphia, can be replicated in other U.S. cities, such as Washington D.C., Boston, Phoenix, and Portland, to assess the equity and efficiency of their transit systems.\nMoreover, the approach holds significant promise for regions with limited data, particularly in South Asia, where detailed transit system information such as GTFS data may not be readily available. Even in such contexts, the methodology can provide valuable insights into the accessibility of transit networks and their effectiveness in reaching low-income and vulnerable communities. By adapting this framework to local data availability and context, it is possible to identify gaps in service provision and opportunities for enhancing public transportation to better serve underserved populations. Ultimately, this approach, which balances social equity and operational efficiency, offers a comprehensive tool for advancing more inclusive, sustainable, and resilient transit solutions globally."
  },
  {
    "objectID": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4_RS_VB.html",
    "href": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4_RS_VB.html",
    "title": "Assignment 4: Street Networks & Web Scraping & Text Analytics",
    "section": "",
    "text": "Riya Saini and Varun Bhakhri\nimport altair as alt\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport osmnx as ox\nfrom shapely.geometry import shape\nimport geopandas as gpd\nimport folium\n\n# Show all columns in dataframes\npd.options.display.max_columns = 999\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");"
  },
  {
    "objectID": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4_RS_VB.html#part-1-visualizing-crash-data-in-philadelphia",
    "href": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4_RS_VB.html#part-1-visualizing-crash-data-in-philadelphia",
    "title": "Assignment 4: Street Networks & Web Scraping & Text Analytics",
    "section": "Part 1: Visualizing crash data in Philadelphia",
    "text": "Part 1: Visualizing crash data in Philadelphia\n\n1.1 Load the geometry for the region being analyzed\nWe’ll analyze crashes in the “Central” planning district in Philadelphia, a rough approximation for Center City. Planning districts can be loaded from Open Data Philly. Read the data into a GeoDataFrame using the following link:\nhttp://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson\nSelect the “Central” district and extract the geometry polygon for only this district. After this part, you should have a polygon variable of type shapely.geometry.polygon.Polygon.\n\ngeojson_file = \"Data/Planning_Districts.geojson\"\ngdf = gpd.read_file(geojson_file)\n\nfiltered_gdf = gdf[gdf[\"DIST_NAME\"] == \"Central\"]\n\n# Extract geometries as Shapely objects\ngeometries = filtered_gdf['geometry'].iloc[0]\n\n\n\n1.2 Get the street network graph\nUse OSMnx to create a network graph (of type ‘drive’) from your polygon boundary in 1.1.\n\ncentral = ox.graph_from_polygon(geometries, network_type='drive')\n\n\n# Remove nodes\nox.plot_graph(central, node_size=0);\n\n\n\n\n\n\n\n\n\n\n1.3 Convert your network graph edges to a GeoDataFrame\nUse OSMnx to create a GeoDataFrame of the network edges in the graph object from part 1.2. The GeoDataFrame should contain the edges but not the nodes from the network.\n\ncentral_edges = ox.graph_to_gdfs(central, edges=True, nodes=False)\n\n\ncentral_edges.plot()\n\n\n\n\n\n\n\n\n\n\n1.4 Load PennDOT crash data\nData for crashes (of all types) for 2020, 2021, and 2022 in Philadelphia County is available at the following path:\n./data/CRASH_PHILADELPHIA_XXXX.csv\nYou should see three separate files in the data/ folder. Use pandas to read each of the CSV files, and combine them into a single dataframe using pd.concat().\nThe data was downloaded for Philadelphia County from here.\n\ncrash1 = pd.read_csv(\"Data/CRASH_PHILADELPHIA_2020.csv\")\ncrash2 = pd.read_csv(\"Data/CRASH_PHILADELPHIA_2021.csv\")\ncrash3 = pd.read_csv(\"Data/CRASH_PHILADELPHIA_2022.csv\")\n\ncrash = pd.concat([crash1, crash2, crash3], ignore_index=True)\n\n\n\n1.5 Convert the crash data to a GeoDataFrame\nYou will need to use the DEC_LAT and DEC_LONG columns for latitude and longitude.\nThe full data dictionary for the data is available here\n\ncrash_gdf = gpd.GeoDataFrame(crash, geometry=gpd.points_from_xy(crash.DEC_LONG, crash.DEC_LAT))\n\n\n\n1.6 Trim the crash data to Center City\n\nGet the boundary of the edges data frame (from part 1.3). Accessing the .geometry.unary_union.convex_hull property will give you a nice outer boundary region.\nTrim the crashes using the within() function of the crash GeoDataFrame to find which crashes are within the boundary.\n\nThere should be about 3,750 crashes within the Central district.\n\ncentral_boundary = central_edges.geometry.unary_union.convex_hull\n\ncentral_crashes = crash_gdf[crash_gdf.geometry.within(central_boundary)]\n\n\n\n1.7 Re-project our data into an approriate CRS\nWe’ll need to find the nearest edge (street) in our graph for each crash. To do this, osmnx will calculate the distance from each crash to the graph edges. For this calculation to be accurate, we need to convert from latitude/longitude\nWe’ll convert the local state plane CRS for Philadelphia, EPSG=2272\n\nTwo steps:\n\nProject the graph object (G) using the ox.project_graph. Run ox.project_graph? to see the documentation for how to convert to a specific CRS.\nProject the crash data using the .to_crs() function.\n\n\ncentral_crashes_geocoded = central_crashes.set_crs(epsg=2272)\n\n\ncentral_crashes_geocoded.plot()\n\n\n\n\n\n\n\n\n\n\n\n1.8 Find the nearest edge for each crash\nSee: ox.distance.nearest_edges(). It takes three arguments:\n\nthe network graph\nthe longitude of your crash data (the x attribute of the geometry column)\nthe latitude of your crash data (the y attribute of the geometry column)\n\nYou will get a numpy array with 3 columns that represent (u, v, key) where each u and v are the node IDs that the edge links together. We will ignore the key value for our analysis.\n\n# Extract X and Y coordinates from the geometry column\nx_coords = central_crashes_geocoded.geometry.x\ny_coords = central_crashes_geocoded.geometry.y\n\nnearest_edges = ox.distance.nearest_edges(central, X=x_coords, Y=y_coords)\n\n\n\n1.9 Calculate the total number of crashes per street\n\nMake a DataFrame from your data from part 1.7 with three columns, u, v, and key (we will only use the u and v columns)\nGroup by u and v and calculate the size\nReset the index and name your size() column as crash_count\n\nAfter this step you should have a DataFrame with three columns: u, v, and crash_count.\n\nnearest_edges_df = pd.DataFrame(nearest_edges, columns=['u', 'v', 'key'])\n\ngrouped_df = nearest_edges_df.groupby(['u', 'v']).size().reset_index(name='crash_count')\n\ngrouped_df.head()\n\n\n\n\n\n\n\n\nu\nv\ncrash_count\n\n\n\n\n0\n109729330\n110216446\n1\n\n\n1\n109729474\n3425014859\n2\n\n\n2\n109729486\n110342146\n4\n\n\n3\n109729673\n11345699563\n1\n\n\n4\n109729699\n11345699562\n3\n\n\n\n\n\n\n\n\n\n1.10 Merge your edges GeoDataFrame and crash count DataFrame\nYou can use pandas to merge them on the u and v columns. This will associate the total crash count with each edge in the street network.\nTips: - Use a left merge where the first argument of the merge is the edges GeoDataFrame. This ensures no edges are removed during the merge. - Use the fillna(0) function to fill in missing crash count values with zero.\n\nmerged_df = central_edges.merge(grouped_df, on=['u', 'v'], how='left')\n\nmerged_df['crash_count'] = merged_df['crash_count'].fillna(0).astype(int)\n\nmerged_df.head()\n\n\n\n\n\n\n\n\nu\nv\nosmid\noneway\nlanes\nname\nhighway\nreversed\nlength\ngeometry\nmaxspeed\nbridge\nref\ntunnel\nwidth\nservice\naccess\njunction\ncrash_count\n\n\n\n\n0\n109727439\n109911666\n132508434\nTrue\n1\nBainbridge Street\nresidential\nFalse\n44.347\nLINESTRING (-75.17104 39.94345, -75.17060 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n\n\n1\n109727448\n109727439\n12109011\nTrue\nNaN\nSouth Colorado Street\nresidential\nFalse\n109.496\nLINESTRING (-75.17125 39.94248, -75.17120 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n\n\n2\n109727448\n110034229\n12159387\nTrue\nNaN\nFitzwater Street\nresidential\nFalse\n91.354\nLINESTRING (-75.17125 39.94248, -75.17129 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n\n\n3\n109727507\n110024052\n193364514\nTrue\nNaN\nCarpenter Street\nresidential\nFalse\n53.208\nLINESTRING (-75.17196 39.93973, -75.17134 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n\n\n4\n109728761\n110274344\n672312336\nTrue\nNaN\nBrown Street\nresidential\nFalse\n58.270\nLINESTRING (-75.17317 39.96951, -75.17250 39.9...\n25 mph\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n\n\n\n\n\n\n\n\n\n1.11 Calculate a “Crash Index”\nLet’s calculate a “crash index” that provides a normalized measure of the crash frequency per street. To do this, we’ll need to:\n\nCalculate the total crash count divided by the street length, using the length column\nPerform a log transformation of the crash/length variable — use numpy’s log10() function\nNormalize the index from 0 to 1 (see the lecture notes for an example of this transformation)\n\nNote: since the crash index involves a log transformation, you should only calculate the index for streets where the crash count is greater than zero.\nAfter this step, you should have a new column in the data frame from 1.9 that includes a column called part 1.9.\n\n# Calculate crash count per unit street length\nmerged_df['crash_per_length'] = merged_df['crash_count'] / merged_df['length']\n\n# Step 2: Perform log transformation\nmerged_df['log_crash_per_length'] = np.log10(merged_df['crash_per_length'].replace(0, np.nan))\n\n# Apply a mask to normalize only the rows where logged crash counts &gt; 0\n\nvalid_mask = merged_df['crash_count'] &gt; 0\nmin_value = merged_df.loc[valid_mask, 'log_crash_per_length'].min()\nmax_value = merged_df.loc[valid_mask, 'log_crash_per_length'].max()\n\nmerged_df['normalized_index'] = np.where(\n    valid_mask,\n    (merged_df['log_crash_per_length'] - min_value) / (max_value - min_value), np.nan)\n\nmerged_df.head()\n\n\n\n\n\n\n\n\nu\nv\nosmid\noneway\nlanes\nname\nhighway\nreversed\nlength\ngeometry\nmaxspeed\nbridge\nref\ntunnel\nwidth\nservice\naccess\njunction\ncrash_count\ncrash_per_length\nlog_crash_per_length\nnormalized_index\n\n\n\n\n0\n109727439\n109911666\n132508434\nTrue\n1\nBainbridge Street\nresidential\nFalse\n44.347\nLINESTRING (-75.17104 39.94345, -75.17060 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0.0\nNaN\nNaN\n\n\n1\n109727448\n109727439\n12109011\nTrue\nNaN\nSouth Colorado Street\nresidential\nFalse\n109.496\nLINESTRING (-75.17125 39.94248, -75.17120 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0.0\nNaN\nNaN\n\n\n2\n109727448\n110034229\n12159387\nTrue\nNaN\nFitzwater Street\nresidential\nFalse\n91.354\nLINESTRING (-75.17125 39.94248, -75.17129 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0.0\nNaN\nNaN\n\n\n3\n109727507\n110024052\n193364514\nTrue\nNaN\nCarpenter Street\nresidential\nFalse\n53.208\nLINESTRING (-75.17196 39.93973, -75.17134 39.9...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0.0\nNaN\nNaN\n\n\n4\n109728761\n110274344\n672312336\nTrue\nNaN\nBrown Street\nresidential\nFalse\n58.270\nLINESTRING (-75.17317 39.96951, -75.17250 39.9...\n25 mph\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n0.0\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n1.12 Plot a histogram of the crash index values\nUse matplotlib’s hist() function to plot the crash index values from the previous step.\nYou should see that the index values are Gaussian-distributed, providing justification for why we log-transformed!\n\nplt.figure(figsize=(10, 6))\nplt.hist(merged_df['normalized_index'], bins=20, edgecolor='black', alpha=0.7)\n\nplt.title(\"Distribution of Normalized Crash Index\", fontsize=16)\nplt.xlabel(\"Normalized Crash Index\", fontsize=14)\nplt.ylabel(\"Frequency\", fontsize=14)\n\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n1.13 Plot an interactive map of the street networks, colored by the crash index\nYou can use GeoPandas to make an interactive Folium map, coloring the streets by the crash index column.\nTip: if you use the viridis color map, try using a “dark” tile set for better constrast of the colors.\n\n# Ensure GeoDataFrame is in WGS84 CRS for Folium compatibility\ngdf = merged_df.to_crs(epsg=4326)\n\n# Add the 'osmid' column as a string to ensure compatibility with GeoJSON properties\ngdf['osmid'] = gdf['osmid'].astype(str)\n\n# Convert the GeoDataFrame to GeoJSON\ngeo_json_data = gdf.to_json()\n\n# Normalize the crash index for color mapping\nmin_value = gdf['normalized_index'].min()\nmax_value = gdf['normalized_index'].max()\n\n# Define the colormap\ncolormap = matplotlib.cm.get_cmap(\"YlOrRd\")\n\ndef get_color(value):\n    if value is None or value != value:  # Handle NaN values\n        return \"white\"\n    normalized = (value - min_value) / (max_value - min_value) if max_value &gt; min_value else 0\n    rgba = colormap(normalized)\n    return matplotlib.colors.rgb2hex(rgba)\n\n# Create a Folium map\nm = folium.Map(location=[gdf.geometry.centroid.y.mean(), gdf.geometry.centroid.x.mean()], zoom_start=14, tiles=\"CartoDB Positron\")\n\n# Add GeoJSON layer for line coloring\nfolium.GeoJson(\n    geo_json_data,\n    style_function=lambda feature: {\n        \"weight\": 3,                      \n        \"color\": get_color(feature[\"properties\"][\"normalized_index\"]),  \n        \"opacity\": 0.9,                     \n    },\n    tooltip=folium.GeoJsonTooltip(fields=[\"normalized_index\"], aliases=[\"Crash Index:\"])\n).add_to(m)\n\ncolormap_legend = folium.StepColormap(\n    colors=[matplotlib.colors.rgb2hex(colormap(x)) for x in range(256)],\n    vmin=min_value,\n    vmax=max_value,\n    caption=\"Normalized Crash Index\"\n)\ncolormap_legend.add_to(m)\n\nm\n\nC:\\Users\\varun\\AppData\\Local\\Temp\\ipykernel_24984\\480809056.py:15: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n  colormap = matplotlib.cm.get_cmap(\"YlOrRd\")\nC:\\Users\\varun\\AppData\\Local\\Temp\\ipykernel_24984\\480809056.py:26: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  m = folium.Map(location=[gdf.geometry.centroid.y.mean(), gdf.geometry.centroid.x.mean()], zoom_start=14, tiles=\"CartoDB Positron\")\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nMost crashes appear to occur on highways and along the Schyukill river park."
  },
  {
    "objectID": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4_RS_VB.html#part-2-scraping-craigslist",
    "href": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4_RS_VB.html#part-2-scraping-craigslist",
    "title": "Assignment 4: Street Networks & Web Scraping & Text Analytics",
    "section": "Part 2: Scraping Craigslist",
    "text": "Part 2: Scraping Craigslist\nIn this part, we’ll be extracting information on apartments from Craigslist search results. You’ll be using Selenium and BeautifulSoup to extract the relevant information from the HTML text.\nFor reference on CSS selectors, please see the notes from Week 6.\n\nPrimer: the Craigslist website URL\nWe’ll start with the Philadelphia region. First we need to figure out how to submit a query to Craigslist. As with many websites, one way you can do this is simply by constructing the proper URL and sending it to Craigslist.\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\nThere are three components to this URL.\n\nThe base URL: http://philadelphia.craigslist.org/search/apa\nThe user’s search parameters: ?min_price=1&min_bedrooms=1&minSqft=1\n\n\nWe will send nonzero defaults for some parameters (bedrooms, size, price) in order to exclude results that have empty values for these parameters.\n\n\nThe URL hash: #search=1~gallery~0~0\n\n\nAs we will see later, this part will be important because it contains the search page result number.\n\nThe Craigslist website requires Javascript, so we’ll need to use Selenium to load the page, and then use BeautifulSoup to extract the information we want.\n\n\n2.1 Initialize a selenium driver and open Craigslist\nAs discussed in lecture, you can use Chrome, Firefox, or Edge as your selenium driver. In this part, you should do two things:\n\nInitialize the selenium driver\nUse the driver.get() function to open the following URL:\n\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\nThis will give you the search results for 1-bedroom apartments in Philadelphia.\n\n!pip install selenium \nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport requests\nfrom selenium.webdriver.common.by import By\nimport pandas as pd\n\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: selenium in c:\\programdata\\mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages (4.13.0)\nRequirement already satisfied: urllib3[socks]&lt;3,&gt;=1.26 in c:\\programdata\\mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages (from selenium) (1.26.19)\nRequirement already satisfied: trio~=0.17 in c:\\programdata\\mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages (from selenium) (0.26.2)\nRequirement already satisfied: trio-websocket~=0.9 in c:\\programdata\\mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages (from selenium) (0.11.1)\nRequirement already satisfied: certifi&gt;=2021.10.8 in c:\\programdata\\mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages (from selenium) (2024.7.4)\nRequirement already satisfied: attrs&gt;=23.2.0 in c:\\programdata\\mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages (from trio~=0.17-&gt;selenium) (24.2.0)\nRequirement already satisfied: sortedcontainers in c:\\programdata\\mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages (from trio~=0.17-&gt;selenium) (2.4.0)\nRequirement already satisfied: idna in c:\\programdata\\mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages (from trio~=0.17-&gt;selenium) (3.8)\nRequirement already satisfied: outcome in c:\\programdata\\mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages (from trio~=0.17-&gt;selenium) (1.3.0.post0)\nRequirement already satisfied: sniffio&gt;=1.3.0 in c:\\programdata\\mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages (from trio~=0.17-&gt;selenium) (1.3.1)\nRequirement already satisfied: cffi&gt;=1.14 in c:\\programdata\\mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages (from trio~=0.17-&gt;selenium) (1.17.0)\nRequirement already satisfied: exceptiongroup in c:\\programdata\\mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages (from trio~=0.17-&gt;selenium) (1.2.2)\nRequirement already satisfied: wsproto&gt;=0.14 in c:\\programdata\\mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages (from trio-websocket~=0.9-&gt;selenium) (1.2.0)\nRequirement already satisfied: PySocks!=1.5.7,&lt;2.0,&gt;=1.5.6 in c:\\programdata\\mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages (from urllib3[socks]&lt;3,&gt;=1.26-&gt;selenium) (1.7.1)\nRequirement already satisfied: pycparser in c:\\programdata\\mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages (from cffi&gt;=1.14-&gt;trio~=0.17-&gt;selenium) (2.22)\nRequirement already satisfied: h11&lt;1,&gt;=0.9.0 in c:\\programdata\\mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages (from wsproto&gt;=0.14-&gt;trio-websocket~=0.9-&gt;selenium) (0.14.0)\n\n\n\ndriver = webdriver.Chrome()\n\n\nurl = \"https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\"\ndriver.get(url)\n\n\n\n2.2 Initialize your “soup”\nOnce selenium has the page open, we can get the page source from the driver and use BeautifulSoup to parse it. In this part, initialize a BeautifulSoup object with the driver’s page source\n\npropertySoup = BeautifulSoup(driver.page_source, \"html.parser\")\n\n\n\n2.3 Parsing the HTML\nNow that we have our “soup” object, we can use BeautifulSoup to extract out the elements we need:\n\nUse the Web Inspector to identify the HTML element that holds the information on each apartment listing.\nUse BeautifulSoup to extract these elements from the HTML.\n\nAt the end of this part, you should have a list of 120 elements, where each element is the listing for a specific apartment on the search page.\n\n# the class \"serach-result\" is contained within div.\nelement = propertySoup.select(\"div.cl-search-result\")\nprint(element[0].prettify())\nprint(len(element))\n\n&lt;div class=\"cl-search-result cl-search-view-mode-gallery\" data-pid=\"7805986336\" title=\"NEW TOTALLY-RENOVATED Apt! Great Fishtown LOCATION * All NEW SEE PICS!\"&gt;\n &lt;div class=\"gallery-card\"&gt;\n  &lt;div class=\"cl-gallery\"&gt;\n   &lt;div class=\"gallery-inner\"&gt;\n    &lt;a class=\"main\" href=\"https://philadelphia.craigslist.org/apa/d/philadelphia-new-totally-renovated-apt/7805986336.html\"&gt;\n     &lt;div class=\"swipe\" style=\"visibility: visible;\"&gt;\n      &lt;div class=\"swipe-wrap\" style=\"width: 13824px;\"&gt;\n       &lt;div data-index=\"0\" style=\"width: 384px; left: 0px; transition-duration: 0ms; transform: translateX(0px);\"&gt;\n        &lt;span class=\"loading icom-\"&gt;\n        &lt;/span&gt;\n        &lt;img alt=\"NEW TOTALLY-RENOVATED Apt! Great Fishtown LOCATION * All NEW SEE PICS! 1\" data-image-index=\"0\" src=\"https://images.craigslist.org/00f0f_jq1fIRpuS7u_0CI0pO_600x450.jpg\"/&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"1\" style=\"width: 384px; left: -384px; transition-duration: 0ms; transform: translateX(384px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"2\" style=\"width: 384px; left: -768px; transition-duration: 0ms; transform: translateX(384px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"3\" style=\"width: 384px; left: -1152px; transition-duration: 0ms; transform: translateX(384px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"4\" style=\"width: 384px; left: -1536px; transition-duration: 0ms; transform: translateX(384px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"5\" style=\"width: 384px; left: -1920px; transition-duration: 0ms; transform: translateX(384px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"6\" style=\"width: 384px; left: -2304px; transition-duration: 0ms; transform: translateX(384px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"7\" style=\"width: 384px; left: -2688px; transition-duration: 0ms; transform: translateX(384px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"8\" style=\"width: 384px; left: -3072px; transition-duration: 0ms; transform: translateX(384px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"9\" style=\"width: 384px; left: -3456px; transition-duration: 0ms; transform: translateX(384px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"10\" style=\"width: 384px; left: -3840px; transition-duration: 0ms; transform: translateX(384px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"11\" style=\"width: 384px; left: -4224px; transition-duration: 0ms; transform: translateX(384px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"12\" style=\"width: 384px; left: -4608px; transition-duration: 0ms; transform: translateX(384px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"13\" style=\"width: 384px; left: -4992px; transition-duration: 0ms; transform: translateX(384px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"14\" style=\"width: 384px; left: -5376px; transition-duration: 0ms; transform: translateX(384px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"15\" style=\"width: 384px; left: -5760px; transition-duration: 0ms; transform: translateX(384px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"16\" style=\"width: 384px; left: -6144px; transition-duration: 0ms; transform: translateX(384px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"17\" style=\"width: 384px; left: -6528px; transition-duration: 0ms; transform: translateX(-384px);\"&gt;\n       &lt;/div&gt;\n      &lt;/div&gt;\n     &lt;/div&gt;\n     &lt;div class=\"slider-back-arrow icom-\"&gt;\n     &lt;/div&gt;\n     &lt;div class=\"slider-forward-arrow icom-\"&gt;\n     &lt;/div&gt;\n    &lt;/a&gt;\n   &lt;/div&gt;\n   &lt;div class=\"dots\"&gt;\n    &lt;span class=\"dot selected\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n   &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;a class=\"cl-app-anchor text-only posting-title\" href=\"https://philadelphia.craigslist.org/apa/d/philadelphia-new-totally-renovated-apt/7805986336.html\" tabindex=\"0\"&gt;\n   &lt;span class=\"label\"&gt;\n    NEW TOTALLY-RENOVATED Apt! Great Fishtown LOCATION * All NEW SEE PICS!\n   &lt;/span&gt;\n  &lt;/a&gt;\n  &lt;div class=\"meta-line\"&gt;\n   &lt;button class=\"bd-button cl-favorite-button icon-only\" tabindex=\"0\" title=\"add to favorites list\" type=\"button\"&gt;\n    &lt;span class=\"icon icom-\"&gt;\n    &lt;/span&gt;\n   &lt;/button&gt;\n   &lt;div class=\"meta\"&gt;\n    7 mins ago\n    &lt;span class=\"separator\"&gt;\n    &lt;/span&gt;\n    &lt;span class=\"housing-meta\"&gt;\n     &lt;span class=\"post-bedrooms\"&gt;\n      1br\n     &lt;/span&gt;\n     &lt;span class=\"post-sqft\"&gt;\n      750ft\n      &lt;span class=\"exponent\"&gt;\n       2\n      &lt;/span&gt;\n     &lt;/span&gt;\n    &lt;/span&gt;\n    &lt;span class=\"separator\"&gt;\n    &lt;/span&gt;\n    NE Fishtown / Richmond / Philadelphia\n   &lt;/div&gt;\n   &lt;button class=\"bd-button cl-banish-button icon-only\" tabindex=\"0\" title=\"hide posting\" type=\"button\"&gt;\n    &lt;span class=\"icon icom-\"&gt;\n    &lt;/span&gt;\n   &lt;/button&gt;\n  &lt;/div&gt;\n  &lt;span class=\"priceinfo\"&gt;\n   $1,095\n  &lt;/span&gt;\n &lt;/div&gt;\n&lt;/div&gt;\n\n120\n\n\n\n\n2.4 Find the relevant pieces of information\nWe will now focus on the first element in the list of 120 apartments. Use the prettify() function to print out the HTML for this first element.\nFrom this HTML, identify the HTML elements that hold:\n\nThe apartment price\nThe number of bedrooms\nThe square footage\nThe apartment title\n\nFor the first apartment, print out each of these pieces of information, using BeautifulSoup to select the proper elements.\n\n# From looking at the nested elements we can see that the partment price is listed under &lt;priceinfo&gt;, no. of bedrooms under &lt;post-bedrooms&gt; and so forth. \n# I used the nested HTML structure above and its CSS styling to get to the class element rather than looking at elemnt 0.\napt1 = element[0]\nspans = apt1.select('span')\nprint(f\"apartment price: {apt1.find('span',{'class' : 'priceinfo'}).text}\",end=\"\\n\")\n\nprint(f\"number of bedrooms: {apt1.find('span',{'class' : 'post-bedrooms'}).text[:1]}\",end=\"\\n\")\n\nprint(f\"square footage: {apt1.find('span',{'class' : 'post-sqft'}).text[:4]}\",end=\"\\n\")\n\nprint(f\"apartment title: {apt1.find('span',{'class' : 'label'}).text}\",end=\"\\n\")\n\napartment price: $1,095\nnumber of bedrooms: 1\nsquare footage: 750f\napartment title: NEW TOTALLY-RENOVATED Apt! Great Fishtown LOCATION * All NEW SEE PICS!\n\n\n\n\n2.5 Functions to format the results\nIn this section, you’ll create functions that take in the raw string elements for price, size, and number of bedrooms and returns them formatted as numbers.\nI’ve started the functions to format the values. You should finish theses functions in this section.\nHints - You can use string formatting functions like string.replace() and string.strip() - The int() and float() functions can convert strings to numbers\n\n# I used the string.replace() function along with the .strip() function. In the last line I converted the string to an integer. \ndef format_bedrooms(bedrooms_string):\n    formatted_string = bedrooms_string.replace(\"br\", \"\").strip()\n    return int(formatted_string)\n\n\n# While going through the results, I noticed that the &lt;span&gt; element had 2 class: \"post-sqft\" and \"exponent\". \n# In this instance, I removed \"ft\" and \"2\" using the replace and endswith function and passed the condition through float().\nimport re\ndef format_size(size_string):\n    formatted_string = size_string.replace(\"ft\", \"\").strip()\n    if formatted_string.endswith('2'):\n        formatted_string = formatted_string[:-1]\n    return float(formatted_string)\n\n\n# Similarly, looking at the website and the nested element I looked at possible symbols to be replaced. \n# Using the functions I replaced dollar sign, commas, or spaces and returned the price as a float\ndef format_price(price_string):\n    formatted_string = price_string.replace(\"$\", \"\").replace(\",\", \"\").strip()\n    return float(formatted_string)\n\n\n\n2.6 Putting it all together\nIn this part, you’ll complete the code block below using results from previous parts. The code will loop over 5 pages of search results and scrape data for 600 apartments.\nWe can get a specific page by changing the search=PAGE part of the URL hash. For example, to get page 2 instead of page 1, we will navigate to:\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=2gallery0~0\nIn the code below, the outer for loop will loop over 5 pages of search results. The inner for loop will loop over the 120 apartments listed on each search page.\nFill in the missing pieces of the inner loop using the code from the previous section. We will be able to extract out the relevant pieces of info for each apartment.\nAfter filling in the missing pieces and executing the code cell, you should have a Data Frame called results that holds the data for 600 apartment listings.\n\nNotes\nBe careful if you try to scrape more listings. Craigslist will temporarily ban your IP address (for a very short time) if you scrape too much at once. I’ve added a sleep() function to the for loop to wait 30 seconds between scraping requests.\nIf the for loop gets stuck at the “Processing page X…” step for more than a minute or so, your IP address is probably banned temporarily, and you’ll have to wait a few minutes before trying again.\n\nfrom time import sleep\n\n\nresults = []\n\n# search in batches of 120 for 5 pages\n# NOTE: you will get temporarily banned if running more than ~5 pages or so\n# the API limits are more leninient during off-peak times, and you can try\n# experimenting with more pages\nmax_pages = 5\n\n# The base URL we will be using\nbase_url = \"https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1\"\n\n# loop over each page of search results\nfor page_num in range(1, max_pages + 1):\n    print(f\"Processing page {page_num}...\")\n\n    # Update the URL hash for this page number and make the combined URL\n    url_hash = f\"#search={1}~gallery~0~0\"\n    url = base_url + url_hash\n\n    # Go to the driver and wait for 5 seconds\n    driver.get(url)\n    sleep(5)\n\n    # YOUR CODE: get the list of all apartments\n    # This is the same code from Part 1.2 and 1.3\n    # It should be a list of 120 apartments\n    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n    apts = propertySoup.select(\"div.cl-search-result\")\n    print(\"Number of apartments = \", len(apts))\n\n    # loop over each apartment in the list\n    page_results = []\n    for apt in apts:\n\n        # YOUR CODE: the bedrooms string\n        bedrooms = apt.find(\"span\", {\"class\": \"post-bedrooms\"}).text\n\n        # YOUR CODE: the size string\n        size = apt.find(\"span\", {\"class\": \"post-sqft\"}).text\n\n        # YOUR CODE: the title string\n        title = apt.find(\"span\", {\"class\": \"label\"}).text\n\n        # YOUR CODE: the price string\n        price = apt.find(\"span\", {\"class\": \"priceinfo\"}).text\n        \n\n        # Format using functions from Part 1.5\n        bedrooms = format_bedrooms(bedrooms)\n        size = format_size(size)\n        price = format_price(price)\n\n        # Save the result\n        page_results.append([price, size, bedrooms, title])\n\n    # Create a dataframe and save\n    col_names = [\"price\", \"size\", \"bedrooms\", \"title\"]\n    df = pd.DataFrame(page_results, columns=col_names)\n    results.append(df)\n\n    print(\"sleeping for 10 seconds between calls\")\n    sleep(10)\n\n# Finally, concatenate all the results\nresults = pd.concat(results, axis=0).reset_index(drop=True)\n\nProcessing page 1...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 2...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 3...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 4...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 5...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\n\n\n\n# For the above code, I used the existing code and simplified it to work for all selected elements from the website, not just elemt[0] as in the code.\n# Here I look at the results to make sure the code has worked properly. We can verify here that the element[0] matches the description above. \n# Also there are exactly 600 entries.\nprint(results.head)\n\n&lt;bound method NDFrame.head of       price    size  bedrooms  \\\n0    2400.0  1328.0         2   \n1    1000.0  1636.0         2   \n2    2711.0  1101.0         3   \n3    1850.0   851.0         1   \n4    2425.0  1217.0         2   \n..      ...     ...       ...   \n595  1600.0  1500.0         4   \n596  1600.0  1400.0         3   \n597  1500.0  1450.0         4   \n598  2949.0  1149.0         2   \n599  1550.0  1400.0         3   \n\n                                                 title  \n0                                    302 Camsten Court  \n1                   Unique Slice of Malvern History!!!  \n2    Animal lover? This is the purrfect place for y...  \n3    Cozy 1x1 apartment home with private patio/bal...  \n4                 Fabulous 2 Bedroom 2.5 Bath townhome  \n..                                                 ...  \n595       Great Housing Opportunity Get Approved Now!!  \n596  Move-In Special! Affordable Housing Opportunit...  \n597  Move-In Special! Affordable Housing Opportunit...  \n598  Spacious living with beautiful views of the Ph...  \n599    Affordable Housing Opportunity get approved Now  \n\n[600 rows x 4 columns]&gt;\n\n\n\n\n\n2.7 Plotting the distribution of prices\nUse matplotlib’s hist() function to make two histograms for:\n\nApartment prices\nApartment prices per square foot (price / size)\n\nMake sure to add labels to the respective axes and a title describing the plot.\n\nSide note: rental prices per sq. ft. from Craigslist\nThe histogram of price per sq ft should be centered around ~1.5. Here is a plot of how Philadelphia’s rents compare to the other most populous cities:\n\nSource\n\n# While making the histogram, I ran into an error that there were some inf values. \n# I had tried to resolve this error at the beginning of the part 2.6, however it did not work.\n# To avoid getting the error we remove rows where size is zero or NaN\nresults = results[results['size'] &gt; 0]  \nresults = results.dropna(subset=['size'])  \n\n# Calculating price per square foot\nresults['price_per_sqft'] = results['price'] / results['size']\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\n\n# Plot of apartment prices:\nplt.subplot(1, 2, 1)\nplt.hist(results['price'], bins=30, edgecolor='black', alpha=0.7, color='blue')\nplt.xlabel('Price ($)')\nplt.ylabel('Frequency')\nplt.title('Histogram of Apartment Prices')\n\n# Plot of Apartment Prices per Square Foot\nplt.subplot(1, 2, 2)\nplt.hist(results['price_per_sqft'], bins=20, edgecolor='black', alpha=0.7, color='blue')\nplt.xlabel('Price per Sq. Ft. ($)')\nplt.ylabel('Frequency')\nplt.title('Histogram of Apartment Prices per Sq. Ft.')\nplt.xlim(0, 10)\n\nplt.tight_layout()\nplt.show()\n\n# After resolving several errors with the size column- trailing number and missings, we can see from the plots below that the average apartment price per square ft is ~1.5\n\n\n\n\n\n\n\n\n\n\n\n2.8 Comparing prices for different sizes\nUse altair to explore the relationship between price, size, and number of bedrooms. Make an interactive scatter plot of price (x-axis) vs. size (y-axis), with the points colored by the number of bedrooms.\nMake sure the plot is interactive (zoom-able and pan-able) and add a tooltip with all of the columns in our scraped data frame.\nWith this sort of plot, you can quickly see the outlier apartments in terms of size and price.\n\n# I created an interactive plot where the tooltip displays the title and all other webscrapped information about a listing in Philadelphia.\n# From the plots we can see that price is proportional to the number of bedrooms for the most part. The same holds true for our second plot. \nimport altair as alt\n\n# Plot of price vs. size\nscatter_plot = alt.Chart(results).mark_circle(size=60).encode(\n    x='price',\n    y='size',\n    color='bedrooms:N', \n    tooltip=['price', 'size', 'bedrooms', 'title']  \n).interactive()  \n\n# Plot of price per sqft vs. size\nscatter_plot1 = alt.Chart(results).mark_circle(size=60).encode(\n    x='price_per_sqft',\n    y='size',\n    color='bedrooms:N', \n    tooltip=['price', 'size', 'bedrooms', 'title']  \n).interactive()  \n\n\nfinal_plot = scatter_plot | scatter_plot1 \nfinal_plot"
  },
  {
    "objectID": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4_RS_VB.html#load-data-google-review-data",
    "href": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4_RS_VB.html#load-data-google-review-data",
    "title": "Assignment 4: Street Networks & Web Scraping & Text Analytics",
    "section": "3.1. Load Data Google Review Data",
    "text": "3.1. Load Data Google Review Data\nTo begin, let’s load a dataset of Google reviews for Wissahickon Valley Park. This dataset includes user reviews, ratings, and other information, which we’ll use to analyze visitors’ emotions.\n\nimport pandas as pd\n\n\ndat = pd.read_csv('Data/wissahickon_review.csv')\ndat.head()\n\n\n\n\n\n\n\n\nreview_datetime_utc\nauthor_title\nauthor_id\nreview_text\nreview_rating\n\n\n\n\n0\n04/13/2024 22:24:35\nMatt Mccrum\n103695489705227565879\nBest trails in philly\n5\n\n\n1\n04/12/2024 00:38:20\n??\n108101934735094350531\n👍\n5\n\n\n2\n04/10/2024 18:02:42\nPete Rhein\n118156100062911374661\nThe best wilderness inside Philadelphia. A few...\n5\n\n\n3\n04/08/2024 17:25:08\nLiubomyr\n102226921330036615545\nAmazing for dog walking.\n5\n\n\n4\n04/06/2024 21:20:53\nD Hancock\n106191637112702445838\nFirst: not an easy walk to say the least.\\n\\nR...\n5"
  },
  {
    "objectID": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4_RS_VB.html#apply-emotion-analysis-model",
    "href": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4_RS_VB.html#apply-emotion-analysis-model",
    "title": "Assignment 4: Street Networks & Web Scraping & Text Analytics",
    "section": "3.2. Apply Emotion Analysis Model",
    "text": "3.2. Apply Emotion Analysis Model\nNow, let’s apply an emotion analysis model to understand the emotions conveyed in the reviews. We’ll use a pre-trained model from Hugging Face.\nNote1: Use the link below to access the model or choose another from Hugging Face if you prefer.\nDistilBERT Emotion Model\nNote2: take the label with the highest score as the predicted label for each text.\n\nfrom transformers import pipeline\n\npipe = pipeline(\"text-classification\", model=\"bhadresh-savani/distilbert-base-uncased-emotion\")\n\nC:\\ProgramData\\mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn("
  },
  {
    "objectID": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4_RS_VB.html#visualize-the-emotion-breakdown",
    "href": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4_RS_VB.html#visualize-the-emotion-breakdown",
    "title": "Assignment 4: Street Networks & Web Scraping & Text Analytics",
    "section": "3.3. Visualize the Emotion Breakdown",
    "text": "3.3. Visualize the Emotion Breakdown\nTo gain insights, let’s visualize the breakdown of emotions expressed in the reviews. This will help us understand the general sentiment of park visitors and any prevailing emotions.\n\n#Convert the column to a list of strings:\nreview_texts = dat[\"review_text\"].tolist()\n\n# Initialize the text classification pipeline\nclassifier = pipeline(\"text-classification\", model='bhadresh-savani/distilbert-base-uncased-emotion', top_k=None)\n\n# Get predictions for each review text\npredictions = classifier(review_texts)\n\n\ndat['predicted_label'] = [pred[0]['label'] for pred in predictions]\n\ndat.head()\n\n\n\n\n\n\n\n\nreview_datetime_utc\nauthor_title\nauthor_id\nreview_text\nreview_rating\npredicted_label\n\n\n\n\n0\n04/13/2024 22:24:35\nMatt Mccrum\n103695489705227565879\nBest trails in philly\n5\njoy\n\n\n1\n04/12/2024 00:38:20\n??\n108101934735094350531\n👍\n5\nanger\n\n\n2\n04/10/2024 18:02:42\nPete Rhein\n118156100062911374661\nThe best wilderness inside Philadelphia. A few...\n5\njoy\n\n\n3\n04/08/2024 17:25:08\nLiubomyr\n102226921330036615545\nAmazing for dog walking.\n5\nsurprise\n\n\n4\n04/06/2024 21:20:53\nD Hancock\n106191637112702445838\nFirst: not an easy walk to say the least.\\n\\nR...\n5\njoy\n\n\n\n\n\n\n\n\ngrouped_counts = dat[\"predicted_label\"].value_counts()\n\nplt.figure(figsize=(8, 6))\ngrouped_counts.plot(kind=\"bar\", color='skyblue', edgecolor='black')\n\nplt.title(\"Count of Each Emotion\", fontsize=14)\nplt.xlabel(\"Emotion\", fontsize=12)\nplt.ylabel(\"Count\", fontsize=12)\nplt.xticks(rotation=45, fontsize=10)\nplt.yticks(fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nMost reviews appear to be positive with joy as the clear majority emotion"
  },
  {
    "objectID": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4_RS_VB.html#additional-exploration",
    "href": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4_RS_VB.html#additional-exploration",
    "title": "Assignment 4: Street Networks & Web Scraping & Text Analytics",
    "section": "3.4. Additional Exploration",
    "text": "3.4. Additional Exploration\nYou can explore the data further by creating a word cloud of frequently mentioned words for a specific emotion. For example, if you want to focus on reviews with “joy” as the dominant emotion, what are the words that appear frequently in those reviews?\n\nfiltered_dat = dat[dat[\"predicted_label\"] == \"sadness\"]\n\n# Convert the filtered column into a list\nsadness_list = filtered_dat[\"review_text\"].tolist()\n\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n# Define the function to plot the word cloud\ndef plot_cloud(wordcloud):\n    # Set figure size\n    plt.figure(figsize=(10, 5))\n    # Display image\n    plt.imshow(wordcloud, interpolation='bilinear')\n    # No axis details\n    plt.axis(\"off\")\n    plt.show()\n\n# Join the sadness_list into a single string\ntext = \" \".join(sadness_list)\n\n# Create the WordCloud object\nwordcloud = WordCloud(\n    width=3000,\n    height=2000,\n    random_state=1,\n    background_color='salmon',\n    colormap='Pastel1',\n    max_words=100,\n    collocations=False,\n    stopwords=STOPWORDS\n).generate(text)\n\n# Plot the word cloud\nplot_cloud(wordcloud)\n   \n\n\n\n\n\n\n\n\nRunning the wordcloud for the “sadness” emotion in the reviews highlights issues of trash and getting lost."
  },
  {
    "objectID": "Combined.html",
    "href": "Combined.html",
    "title": "Transit Priority",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport shapely\nimport folium\nfrom folium import Choropleth"
  },
  {
    "objectID": "Combined.html#criteria-for-critical-bus-network-segments-improvements",
    "href": "Combined.html#criteria-for-critical-bus-network-segments-improvements",
    "title": "Transit Priority",
    "section": "Criteria for Critical Bus Network Segments Improvements",
    "text": "Criteria for Critical Bus Network Segments Improvements\nTo prioritize bus transit solutions effectively, we established the following criteria for determining high-priority areas:\nTransit Vulnerability Index: Areas must have a Transit Vulnerability Index (TVI) score of 4 or higher, highlighting significant levels of social vulnerability.\n\nHeadway: Routes with headways under 2 minutes are considered, ensuring focus on high-frequency services where improvements can maximize impact.\n\nRidership: Only routes within the upper quartile of ridership are included, reflecting areas of high demand and usage.\nThese criteria allow us to target interventions where they are most needed, addressing equity, efficiency, and demand in a data-driven manner.\n\nupper_quartile = final_segments[\"Ridership\"].quantile(0.25)\n\nfiltered_segments = final_segments[\n    (final_segments[\"Social_Vulnerability_Index\"] &gt;= 3) &  \n    (final_segments[\"headway\"] &lt; 5) &                    \n    (final_segments[\"Ridership\"] &gt;= upper_quartile)]\n\nfiltered_segments.head()\n\n\n\n\n\n\n\n\npm_index_right\nstop_id\nbus_arrivals\nfrequency\nheadway\nstop_name\nstop_lat\nstop_lon\nlocation_type\nparent_station\n...\nname10\nn_veryhigh\nyear\nhsi_score\nhei_score\nhvi_score\nobjectid\nShape__Area\nShape__Length\nSocial_Vulnerability_Index\n\n\n\n\n16654\n4767.0\n15973.0\n46.0\n15.333333\n3.913043\nBroad St & Wyoming Av\n40.024676\n-75.147927\nNaN\nNaN\n...\n280.0\n1.0\n2023.0\n6.741606\n0.974944\n4.672679\n344.0\n1.226439e+06\n4682.828235\n6.0\n\n\n16389\n4767.0\n15973.0\n46.0\n15.333333\n3.913043\nBroad St & Wyoming Av\n40.024676\n-75.147927\nNaN\nNaN\n...\n280.0\n1.0\n2023.0\n6.741606\n0.974944\n4.672679\n344.0\n1.226439e+06\n4682.828235\n6.0\n\n\n16388\n4767.0\n15973.0\n46.0\n15.333333\n3.913043\nBroad St & Wyoming Av\n40.024676\n-75.147927\nNaN\nNaN\n...\n280.0\n1.0\n2023.0\n6.741606\n0.974944\n4.672679\n344.0\n1.226439e+06\n4682.828235\n6.0\n\n\n16386\n4767.0\n15973.0\n46.0\n15.333333\n3.913043\nBroad St & Wyoming Av\n40.024676\n-75.147927\nNaN\nNaN\n...\n280.0\n1.0\n2023.0\n6.741606\n0.974944\n4.672679\n344.0\n1.226439e+06\n4682.828235\n6.0\n\n\n16653\n4767.0\n15973.0\n46.0\n15.333333\n3.913043\nBroad St & Wyoming Av\n40.024676\n-75.147927\nNaN\nNaN\n...\n280.0\n1.0\n2023.0\n6.741606\n0.974944\n4.672679\n344.0\n1.226439e+06\n4682.828235\n6.0\n\n\n\n\n5 rows × 49 columns\n\n\n\n\nVisualising the most important segments for improvements:\nThe segments selected based on the criteria outlined above are concentrated along major streets in Philadelphia, including Broad Street, Market Street, Kensington Avenue, and Roosevelt Boulevard. High ridership on these segments likely reflects passenger transfers from nearby subway stations.\n\nfrom shapely.geometry import Point\n\n# Re-project to a projected CRS for accurate centroid calculation\nprojected_segments = filtered_segments.to_crs(epsg=3857)\n\n# Calculate the mean centroid coordinates in the projected CRS\nmean_y = projected_segments.geometry.centroid.y.mean()\nmean_x = projected_segments.geometry.centroid.x.mean()\n\n# Convert the mean centroid coordinates back to WGS84 for Folium\nmean_point = gpd.GeoSeries([Point(mean_x, mean_y)], crs=3857).to_crs(epsg=4326)\nmean_lat, mean_lon = mean_point.geometry[0].y, mean_point.geometry[0].x\n\n# Initialize a Folium map centered around the data\nm = folium.Map(\n    location=[mean_lat, mean_lon],\n    zoom_start=11,\n    tiles='CartoDB Positron')\n\n# choropleth map for TVI\nfolium.Choropleth(\n    geo_data=vulnerability,\n    name=\"Social Vulnerability Score\",\n    data=vulnerability,\n    columns=[\"GEOID\", \"Social_Vulnerability_Index\"],\n    key_on=\"feature.properties.GEOID\", \n    fill_color=\"YlOrRd\",\n    fill_opacity=0.3,\n    line_opacity=0.1,\n    legend_name=\"Social Vulnerability Score\",\n).add_to(m)\n\nfolium.GeoJson(\n    bus_network_philadelphia,\n    style_function=lambda x: {\"color\": \"grey\", \"weight\": 0.5, \"opacity\": 0.4},\n    name=\"Bus Network (Philadelphia)\"\n).add_to(m)\n\nfolium.GeoJson(\n    filtered_segments,\n    style_function=lambda x: {\"color\": \"blue\", \"weight\": 3, \"opacity\": 1},\n    name=\"Filtered Segments\"\n).add_to(m)\n\nfolium.LayerControl().add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nThe table below showcases the top 10 bus segments ranked by ridership. These segments represent the busiest parts of the network, where high passenger volumes underscore their critical role in Philadelphia’s transit system. Prioritizing improvements in these areas is essential to enhance service reliability and meet the needs of the city’s most active transit users. By focusing on these high-demand corridors, transit agencies can address existing inefficiencies and better align services with the needs of vulnerable communities.\n\n# Sort the GeoDataFrame by the 'headway' column in ascending order\nfiltered_segments = filtered_segments.sort_values(by='headway', ascending=True)\n\n# Group by 'stop_name' and calculate aggregate values for the selected columns\nfinal_streets = (\n    filtered_segments.groupby(\"stop_name\")[[\"headway\", \"frequency\", \"Ridership\", \"Social_Vulnerability_Index\"]]\n    .mean() \n    .reset_index())\n\nfinal_streets.rename(columns={\"Social_Vulnerability_Index\": \"Transit_Rider_Vulnerability_Index\"}, inplace=True)\nfinal_streets = final_streets.sort_values(by='Ridership', ascending=False)\nfinal_streets.head(10)\n\n\n\n\n\n\n\n\nstop_name\nheadway\nfrequency\nRidership\nTransit_Rider_Vulnerability_Index\n\n\n\n\n10\nArrott Transportation Center\n2.903226\n20.666667\n1550.0\n3.000000\n\n\n1\n23rd St & Venango St Loop\n1.855670\n32.333333\n1503.0\n4.000000\n\n\n26\nBroad St & Olney Av - FS\n3.750000\n16.000000\n825.0\n3.750000\n\n\n63\nRoosevelt Blvd & Broad St - FS\n3.913043\n15.333333\n666.0\n3.000000\n\n\n30\nBroad St & Tabor Rd\n3.461538\n17.333333\n665.0\n3.285714\n\n\n47\nMarket St & 40th St\n4.090909\n14.666667\n575.0\n3.000000\n\n\n2\n33rd St & Dauphin St Loop\n4.864865\n12.333333\n536.0\n4.000000\n\n\n35\nFrankford Av & Margaret St\n3.529412\n17.000000\n492.0\n3.000000\n\n\n73\nWoodland Av & 50th St\n3.600000\n16.666667\n481.0\n5.000000\n\n\n18\nBroad St & Allegheny Av\n3.471429\n17.333333\n415.0\n4.000000"
  },
  {
    "objectID": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4.html",
    "href": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4.html",
    "title": "Assignment 4: Street Networks & Web Scraping & Text Analytics",
    "section": "",
    "text": "Part 1: Visualizing crash data in Philadelphia\nIn this section, you will use osmnx to analyze the crash incidence in Center City.\nPart 2: Scraping Craigslist\nIn this section, you will use Selenium and BeautifulSoup to scrape data for hundreds of apartments from Philadelphia’s Craigslist portal.\nPart 3: Text Analytics"
  },
  {
    "objectID": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4.html#part-1-visualizing-crash-data-in-philadelphia",
    "href": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4.html#part-1-visualizing-crash-data-in-philadelphia",
    "title": "Assignment 4: Street Networks & Web Scraping & Text Analytics",
    "section": "Part 1: Visualizing crash data in Philadelphia",
    "text": "Part 1: Visualizing crash data in Philadelphia\n\n1.1 Load the geometry for the region being analyzed\nWe’ll analyze crashes in the “Central” planning district in Philadelphia, a rough approximation for Center City. Planning districts can be loaded from Open Data Philly. Read the data into a GeoDataFrame using the following link:\nhttp://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson\nSelect the “Central” district and extract the geometry polygon for only this district. After this part, you should have a polygon variable of type shapely.geometry.polygon.Polygon.\n\n\n1.2 Get the street network graph\nUse OSMnx to create a network graph (of type ‘drive’) from your polygon boundary in 1.1.\n\n\n1.3 Convert your network graph edges to a GeoDataFrame\nUse OSMnx to create a GeoDataFrame of the network edges in the graph object from part 1.2. The GeoDataFrame should contain the edges but not the nodes from the network.\n\n\n1.4 Load PennDOT crash data\nData for crashes (of all types) for 2020, 2021, and 2022 in Philadelphia County is available at the following path:\n./data/CRASH_PHILADELPHIA_XXXX.csv\nYou should see three separate files in the data/ folder. Use pandas to read each of the CSV files, and combine them into a single dataframe using pd.concat().\nThe data was downloaded for Philadelphia County from here.\n\n\n1.5 Convert the crash data to a GeoDataFrame\nYou will need to use the DEC_LAT and DEC_LONG columns for latitude and longitude.\nThe full data dictionary for the data is available here\n\n\n1.6 Trim the crash data to Center City\n\nGet the boundary of the edges data frame (from part 1.3). Accessing the .geometry.unary_union.convex_hull property will give you a nice outer boundary region.\nTrim the crashes using the within() function of the crash GeoDataFrame to find which crashes are within the boundary.\n\nThere should be about 3,750 crashes within the Central district.\n\n\n1.7 Re-project our data into an approriate CRS\nWe’ll need to find the nearest edge (street) in our graph for each crash. To do this, osmnx will calculate the distance from each crash to the graph edges. For this calculation to be accurate, we need to convert from latitude/longitude\nWe’ll convert the local state plane CRS for Philadelphia, EPSG=2272\n\nTwo steps:\n\nProject the graph object (G) using the ox.project_graph. Run ox.project_graph? to see the documentation for how to convert to a specific CRS.\nProject the crash data using the .to_crs() function.\n\n\n\n\n1.8 Find the nearest edge for each crash\nSee: ox.distance.nearest_edges(). It takes three arguments:\n\nthe network graph\nthe longitude of your crash data (the x attribute of the geometry column)\nthe latitude of your crash data (the y attribute of the geometry column)\n\nYou will get a numpy array with 3 columns that represent (u, v, key) where each u and v are the node IDs that the edge links together. We will ignore the key value for our analysis.\n\n\n1.9 Calculate the total number of crashes per street\n\nMake a DataFrame from your data from part 1.7 with three columns, u, v, and key (we will only use the u and v columns)\nGroup by u and v and calculate the size\nReset the index and name your size() column as crash_count\n\nAfter this step you should have a DataFrame with three columns: u, v, and crash_count.\n\n\n1.10 Merge your edges GeoDataFrame and crash count DataFrame\nYou can use pandas to merge them on the u and v columns. This will associate the total crash count with each edge in the street network.\nTips: - Use a left merge where the first argument of the merge is the edges GeoDataFrame. This ensures no edges are removed during the merge. - Use the fillna(0) function to fill in missing crash count values with zero.\n\n\n1.11 Calculate a “Crash Index”\nLet’s calculate a “crash index” that provides a normalized measure of the crash frequency per street. To do this, we’ll need to:\n\nCalculate the total crash count divided by the street length, using the length column\nPerform a log transformation of the crash/length variable — use numpy’s log10() function\nNormalize the index from 0 to 1 (see the lecture notes for an example of this transformation)\n\nNote: since the crash index involves a log transformation, you should only calculate the index for streets where the crash count is greater than zero.\nAfter this step, you should have a new column in the data frame from 1.9 that includes a column called part 1.9.\n\n\n1.12 Plot a histogram of the crash index values\nUse matplotlib’s hist() function to plot the crash index values from the previous step.\nYou should see that the index values are Gaussian-distributed, providing justification for why we log-transformed!\n\n\n1.13 Plot an interactive map of the street networks, colored by the crash index\nYou can use GeoPandas to make an interactive Folium map, coloring the streets by the crash index column.\nTip: if you use the viridis color map, try using a “dark” tile set for better constrast of the colors."
  },
  {
    "objectID": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4.html#part-2-scraping-craigslist",
    "href": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4.html#part-2-scraping-craigslist",
    "title": "Assignment 4: Street Networks & Web Scraping & Text Analytics",
    "section": "Part 2: Scraping Craigslist",
    "text": "Part 2: Scraping Craigslist\nIn this part, we’ll be extracting information on apartments from Craigslist search results. You’ll be using Selenium and BeautifulSoup to extract the relevant information from the HTML text.\nFor reference on CSS selectors, please see the notes from Week 6.\n\nPrimer: the Craigslist website URL\nWe’ll start with the Philadelphia region. First we need to figure out how to submit a query to Craigslist. As with many websites, one way you can do this is simply by constructing the proper URL and sending it to Craigslist.\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\nThere are three components to this URL.\n\nThe base URL: http://philadelphia.craigslist.org/search/apa\nThe user’s search parameters: ?min_price=1&min_bedrooms=1&minSqft=1\n\n\nWe will send nonzero defaults for some parameters (bedrooms, size, price) in order to exclude results that have empty values for these parameters.\n\n\nThe URL hash: #search=1~gallery~0~0\n\n\nAs we will see later, this part will be important because it contains the search page result number.\n\nThe Craigslist website requires Javascript, so we’ll need to use Selenium to load the page, and then use BeautifulSoup to extract the information we want.\n\n\n2.1 Initialize a selenium driver and open Craigslist\nAs discussed in lecture, you can use Chrome, Firefox, or Edge as your selenium driver. In this part, you should do two things:\n\nInitialize the selenium driver\nUse the driver.get() function to open the following URL:\n\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\nThis will give you the search results for 1-bedroom apartments in Philadelphia.\n\n!pip install selenium \nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport requests\nfrom selenium.webdriver.common.by import By\nimport pandas as pd\n\nCollecting selenium\n  Obtaining dependency information for selenium from https://files.pythonhosted.org/packages/91/08/10cff8463b3510b78f9e3dcef6b37c542b06d71ed1240a8940ba0c75d3bc/selenium-4.26.1-py3-none-any.whl.metadata\n  Downloading selenium-4.26.1-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: urllib3[socks]&lt;3,&gt;=1.26 in /Users/delmelle/miniforge3/lib/python3.10/site-packages (from selenium) (2.0.4)\nCollecting trio~=0.17 (from selenium)\n  Obtaining dependency information for trio~=0.17 from https://files.pythonhosted.org/packages/3c/83/ec3196c360afffbc5b342ead48d1eb7393dd74fa70bca75d33905a86f211/trio-0.27.0-py3-none-any.whl.metadata\n  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\nCollecting trio-websocket~=0.9 (from selenium)\n  Obtaining dependency information for trio-websocket~=0.9 from https://files.pythonhosted.org/packages/48/be/a9ae5f50cad5b6f85bd2574c2c923730098530096e170c1ce7452394d7aa/trio_websocket-0.11.1-py3-none-any.whl.metadata\n  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: certifi&gt;=2021.10.8 in /Users/delmelle/miniforge3/lib/python3.10/site-packages (from selenium) (2024.7.4)\nCollecting typing_extensions~=4.9 (from selenium)\n  Obtaining dependency information for typing_extensions~=4.9 from https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl.metadata\n  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\nCollecting websocket-client~=1.8 (from selenium)\n  Obtaining dependency information for websocket-client~=1.8 from https://files.pythonhosted.org/packages/5a/84/44687a29792a70e111c5c477230a72c4b957d88d16141199bf9acb7537a3/websocket_client-1.8.0-py3-none-any.whl.metadata\n  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\nCollecting attrs&gt;=23.2.0 (from trio~=0.17-&gt;selenium)\n  Obtaining dependency information for attrs&gt;=23.2.0 from https://files.pythonhosted.org/packages/6a/21/5b6702a7f963e95456c0de2d495f67bf5fd62840ac655dc451586d23d39a/attrs-24.2.0-py3-none-any.whl.metadata\n  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: sortedcontainers in /Users/delmelle/miniforge3/lib/python3.10/site-packages (from trio~=0.17-&gt;selenium) (2.4.0)\nRequirement already satisfied: idna in /Users/delmelle/miniforge3/lib/python3.10/site-packages (from trio~=0.17-&gt;selenium) (3.4)\nCollecting outcome (from trio~=0.17-&gt;selenium)\n  Obtaining dependency information for outcome from https://files.pythonhosted.org/packages/55/8b/5ab7257531a5d830fc8000c476e63c935488d74609b50f9384a643ec0a62/outcome-1.3.0.post0-py2.py3-none-any.whl.metadata\n  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: sniffio&gt;=1.3.0 in /Users/delmelle/miniforge3/lib/python3.10/site-packages (from trio~=0.17-&gt;selenium) (1.3.0)\nRequirement already satisfied: exceptiongroup in /Users/delmelle/miniforge3/lib/python3.10/site-packages (from trio~=0.17-&gt;selenium) (1.2.0)\nCollecting wsproto&gt;=0.14 (from trio-websocket~=0.9-&gt;selenium)\n  Obtaining dependency information for wsproto&gt;=0.14 from https://files.pythonhosted.org/packages/78/58/e860788190eba3bcce367f74d29c4675466ce8dddfba85f7827588416f01/wsproto-1.2.0-py3-none-any.whl.metadata\n  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\nRequirement already satisfied: pysocks!=1.5.7,&lt;2.0,&gt;=1.5.6 in /Users/delmelle/miniforge3/lib/python3.10/site-packages (from urllib3[socks]&lt;3,&gt;=1.26-&gt;selenium) (1.7.1)\nCollecting h11&lt;1,&gt;=0.9.0 (from wsproto&gt;=0.14-&gt;trio-websocket~=0.9-&gt;selenium)\n  Obtaining dependency information for h11&lt;1,&gt;=0.9.0 from https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata\n  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\nDownloading selenium-4.26.1-py3-none-any.whl (9.7 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.7/9.7 MB 2.9 MB/s eta 0:00:0000:0100:01\nDownloading trio-0.27.0-py3-none-any.whl (481 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 481.7/481.7 kB 4.0 MB/s eta 0:00:0000:0100:01\nDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\nDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nDownloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 1.2 MB/s eta 0:00:00ta 0:00:01\nDownloading attrs-24.2.0-py3-none-any.whl (63 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.0/63.0 kB 2.2 MB/s eta 0:00:00\nDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\nDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\nDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 1.6 MB/s eta 0:00:00\nInstalling collected packages: websocket-client, typing_extensions, h11, attrs, wsproto, outcome, trio, trio-websocket, selenium\n  Attempting uninstall: websocket-client\n    Found existing installation: websocket-client 1.7.0\n    Uninstalling websocket-client-1.7.0:\n      Successfully uninstalled websocket-client-1.7.0\n  Attempting uninstall: typing_extensions\n    Found existing installation: typing_extensions 4.8.0\n    Uninstalling typing_extensions-4.8.0:\n      Successfully uninstalled typing_extensions-4.8.0\n  Attempting uninstall: attrs\n    Found existing installation: attrs 23.1.0\n    Uninstalling attrs-23.1.0:\n      Successfully uninstalled attrs-23.1.0\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngeoviews 1.13.0 requires bokeh&lt;3.6.0,&gt;=3.5.0, but you have bokeh 2.4.3 which is incompatible.\nholoviews 1.19.1 requires bokeh&gt;=3.1, but you have bokeh 2.4.3 which is incompatible.\njupyter-server 2.12.5 requires tornado&gt;=6.2.0, but you have tornado 6.1 which is incompatible.\njupyterlab 4.0.12 requires tornado&gt;=6.2.0, but you have tornado 6.1 which is incompatible.\nnotebook 7.0.7 requires tornado&gt;=6.2.0, but you have tornado 6.1 which is incompatible.\npanel 1.5.0 requires bokeh&lt;3.6.0,&gt;=3.5.0, but you have bokeh 2.4.3 which is incompatible.\nSuccessfully installed attrs-24.2.0 h11-0.14.0 outcome-1.3.0.post0 selenium-4.26.1 trio-0.27.0 trio-websocket-0.11.1 typing_extensions-4.12.2 websocket-client-1.8.0 wsproto-1.2.0\n\n\n\ndriver = webdriver.Chrome()\n\n\nurl = \"https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\"\ndriver.get(url)\n\n\n\n2.2 Initialize your “soup”\nOnce selenium has the page open, we can get the page source from the driver and use BeautifulSoup to parse it. In this part, initialize a BeautifulSoup object with the driver’s page source\n\npropertySoup = BeautifulSoup(driver.page_source, \"html.parser\")\n\n\n\n2.3 Parsing the HTML\nNow that we have our “soup” object, we can use BeautifulSoup to extract out the elements we need:\n\nUse the Web Inspector to identify the HTML element that holds the information on each apartment listing.\nUse BeautifulSoup to extract these elements from the HTML.\n\nAt the end of this part, you should have a list of 120 elements, where each element is the listing for a specific apartment on the search page.\n\nelement = propertySoup.select(\"li.cl-search-result\")\nprint(element[0].prettify())\nprint(len(element))\n\n&lt;li class=\"cl-search-result cl-search-view-mode-gallery\" data-pid=\"7800059598\" title=\"Beautiful Single family Home - Cedar Park University City\"&gt;\n &lt;div class=\"gallery-card\"&gt;\n  &lt;div class=\"cl-gallery\"&gt;\n   &lt;div class=\"gallery-inner\"&gt;\n    &lt;a class=\"main\" href=\"https://philadelphia.craigslist.org/apa/d/philadelphia-beautiful-single-family/7800059598.html\"&gt;\n     &lt;div class=\"swipe\" style=\"visibility: visible;\"&gt;\n      &lt;div class=\"swipe-wrap\" style=\"width: 10336px;\"&gt;\n       &lt;div data-index=\"0\" style=\"width: 304px; left: 0px; transition-duration: 0ms; transform: translateX(0px);\"&gt;\n        &lt;span class=\"loading icom-\"&gt;\n        &lt;/span&gt;\n        &lt;img alt=\"Beautiful Single family Home - Cedar Park University City 1\" data-image-index=\"0\" src=\"https://images.craigslist.org/00N0N_kgvrS202Ypp_0t20CI_600x450.jpg\"/&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"1\" style=\"width: 304px; left: -304px; transition-duration: 0ms; transform: translateX(304px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"2\" style=\"width: 304px; left: -608px; transition-duration: 0ms; transform: translateX(304px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"3\" style=\"width: 304px; left: -912px; transition-duration: 0ms; transform: translateX(304px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"4\" style=\"width: 304px; left: -1216px; transition-duration: 0ms; transform: translateX(304px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"5\" style=\"width: 304px; left: -1520px; transition-duration: 0ms; transform: translateX(304px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"6\" style=\"width: 304px; left: -1824px; transition-duration: 0ms; transform: translateX(304px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"7\" style=\"width: 304px; left: -2128px; transition-duration: 0ms; transform: translateX(304px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"8\" style=\"width: 304px; left: -2432px; transition-duration: 0ms; transform: translateX(304px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"9\" style=\"width: 304px; left: -2736px; transition-duration: 0ms; transform: translateX(304px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"10\" style=\"width: 304px; left: -3040px; transition-duration: 0ms; transform: translateX(304px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"11\" style=\"width: 304px; left: -3344px; transition-duration: 0ms; transform: translateX(304px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"12\" style=\"width: 304px; left: -3648px; transition-duration: 0ms; transform: translateX(304px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"13\" style=\"width: 304px; left: -3952px; transition-duration: 0ms; transform: translateX(304px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"14\" style=\"width: 304px; left: -4256px; transition-duration: 0ms; transform: translateX(304px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"15\" style=\"width: 304px; left: -4560px; transition-duration: 0ms; transform: translateX(304px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"16\" style=\"width: 304px; left: -4864px; transition-duration: 0ms; transform: translateX(-304px);\"&gt;\n       &lt;/div&gt;\n      &lt;/div&gt;\n     &lt;/div&gt;\n     &lt;div class=\"slider-back-arrow icom-\"&gt;\n     &lt;/div&gt;\n     &lt;div class=\"slider-forward-arrow icom-\"&gt;\n     &lt;/div&gt;\n    &lt;/a&gt;\n   &lt;/div&gt;\n   &lt;div class=\"dots\"&gt;\n    &lt;span class=\"dot selected\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n   &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;a class=\"cl-app-anchor text-only posting-title\" href=\"https://philadelphia.craigslist.org/apa/d/philadelphia-beautiful-single-family/7800059598.html\" tabindex=\"0\"&gt;\n   &lt;span class=\"label\"&gt;\n    Beautiful Single family Home - Cedar Park University City\n   &lt;/span&gt;\n  &lt;/a&gt;\n  &lt;div class=\"meta\"&gt;\n   11 mins ago\n   &lt;span class=\"separator\"&gt;\n   &lt;/span&gt;\n   &lt;span class=\"housing-meta\"&gt;\n    &lt;span class=\"post-bedrooms\"&gt;\n     3br\n    &lt;/span&gt;\n    &lt;span class=\"post-sqft\"&gt;\n     1200ft\n     &lt;span class=\"exponent\"&gt;\n      2\n     &lt;/span&gt;\n    &lt;/span&gt;\n   &lt;/span&gt;\n   &lt;span class=\"separator\"&gt;\n   &lt;/span&gt;\n   Cedar park Philadelphia University City\n  &lt;/div&gt;\n  &lt;span class=\"priceinfo\"&gt;\n   $2,200\n  &lt;/span&gt;\n  &lt;button class=\"bd-button cl-favorite-button icon-only\" tabindex=\"0\" title=\"add to favorites list\" type=\"button\"&gt;\n   &lt;span class=\"icon icom-\"&gt;\n   &lt;/span&gt;\n  &lt;/button&gt;\n  &lt;button class=\"bd-button cl-banish-button icon-only\" tabindex=\"0\" title=\"hide posting\" type=\"button\"&gt;\n   &lt;span class=\"icon icom-\"&gt;\n   &lt;/span&gt;\n  &lt;/button&gt;\n &lt;/div&gt;\n&lt;/li&gt;\n\n120\n\n\n\n\n2.4 Find the relevant pieces of information\nWe will now focus on the first element in the list of 120 apartments. Use the prettify() function to print out the HTML for this first element.\nFrom this HTML, identify the HTML elements that hold:\n\nThe apartment price\nThe number of bedrooms\nThe square footage\nThe apartment title\n\nFor the first apartment, print out each of these pieces of information, using BeautifulSoup to select the proper elements.\n\napt1 = element[0]\nspans = apt1.select('span')\nprint(f\"apartment price: {apt1.find('span',{'class' : 'priceinfo'}).text}\",end=\"\\n\")\n\nprint(f\"number of bedrooms: {apt1.find('span',{'class' : 'post-bedrooms'}).text[:1]}\",end=\"\\n\")\n\nprint(f\"square footage: {apt1.find('span',{'class' : 'post-sqft'}).text[:3]}\",end=\"\\n\")\n\nprint(f\"apartment title: {apt1.find('span',{'class' : 'label'}).text}\",end=\"\\n\")\n\napartment price: $2,200\nnumber of bedrooms: 3\nsquare footage: 120\napartment title: Beautiful Single family Home - Cedar Park University City\n\n\n\n\n2.5 Functions to format the results\nIn this section, you’ll create functions that take in the raw string elements for price, size, and number of bedrooms and returns them formatted as numbers.\nI’ve started the functions to format the values. You should finish theses functions in this section.\nHints - You can use string formatting functions like string.replace() and string.strip() - The int() and float() functions can convert strings to numbers\n\ndef format_bedrooms(bedrooms_string):\n    # Format the bedrooms string and return an int\n    # \n    # This will involve using the string.replace() function to \n    # remove unwanted characters\n    \n    return\n\n\ndef format_size(size_string):\n    # Format the size string and return a float\n    # \n    # This will involve using the string.replace() function to \n    # remove unwanted characters\n    \n    return \n\n\ndef format_price(price_string):\n    # Format the price string and return a float\n    # \n    # This will involve using the string.strip() function to \n    # remove unwanted characters\n    return \n\n\n\n2.6 Putting it all together\nIn this part, you’ll complete the code block below using results from previous parts. The code will loop over 5 pages of search results and scrape data for 600 apartments.\nWe can get a specific page by changing the search=PAGE part of the URL hash. For example, to get page 2 instead of page 1, we will navigate to:\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=2gallery0~0\nIn the code below, the outer for loop will loop over 5 pages of search results. The inner for loop will loop over the 120 apartments listed on each search page.\nFill in the missing pieces of the inner loop using the code from the previous section. We will be able to extract out the relevant pieces of info for each apartment.\nAfter filling in the missing pieces and executing the code cell, you should have a Data Frame called results that holds the data for 600 apartment listings.\n\nNotes\nBe careful if you try to scrape more listings. Craigslist will temporarily ban your IP address (for a very short time) if you scrape too much at once. I’ve added a sleep() function to the for loop to wait 30 seconds between scraping requests.\nIf the for loop gets stuck at the “Processing page X…” step for more than a minute or so, your IP address is probably banned temporarily, and you’ll have to wait a few minutes before trying again.\n\nfrom time import sleep\n\n\nresults = []\n\n# search in batches of 120 for 5 pages\n# NOTE: you will get temporarily banned if running more than ~5 pages or so\n# the API limits are more leninient during off-peak times, and you can try\n# experimenting with more pages\nmax_pages = 5\n\n# The base URL we will be using\nbase_url = \"https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1\"\n\n# loop over each page of search results\nfor page_num in range(1, max_pages + 1):\n    print(f\"Processing page {page_num}...\")\n\n    # Update the URL hash for this page number and make the combined URL\n    url_hash = f\"#search={page_num}~gallery~0~0\"\n    url = base_url + url_hash\n\n    # Go to the driver and wait for 5 seconds\n    driver.get(url)\n    sleep(5)\n\n    # YOUR CODE: get the list of all apartments\n    # This is the same code from Part 1.2 and 1.3\n    # It should be a list of 120 apartments\n    soup = \n    apts = \n    print(\"Number of apartments = \", len(apts))\n\n    # loop over each apartment in the list\n    page_results = []\n    for apt in apts:\n\n        # YOUR CODE: the bedrooms string\n        bedrooms = \n\n        # YOUR CODE: the size string\n        size = \n\n        # YOUR CODE: the title string\n        title = \n\n        # YOUR CODE: the price string\n        price = \n\n\n        # Format using functions from Part 1.5\n        bedrooms = format_bedrooms(bedrooms)\n        size = format_size(size)\n        price = format_price(price)\n\n        # Save the result\n        page_results.append([price, size, bedrooms, title])\n\n    # Create a dataframe and save\n    col_names = [\"price\", \"size\", \"bedrooms\", \"title\"]\n    df = pd.DataFrame(page_results, columns=col_names)\n    results.append(df)\n\n    print(\"sleeping for 10 seconds between calls\")\n    sleep(10)\n\n# Finally, concatenate all the results\nresults = pd.concat(results, axis=0).reset_index(drop=True)\n\n\n\n\n2.7 Plotting the distribution of prices\nUse matplotlib’s hist() function to make two histograms for:\n\nApartment prices\nApartment prices per square foot (price / size)\n\nMake sure to add labels to the respective axes and a title describing the plot.\n\nSide note: rental prices per sq. ft. from Craigslist\nThe histogram of price per sq ft should be centered around ~1.5. Here is a plot of how Philadelphia’s rents compare to the other most populous cities:\n\nSource\n\n\n\n2.8 Comparing prices for different sizes\nUse altair to explore the relationship between price, size, and number of bedrooms. Make an interactive scatter plot of price (x-axis) vs. size (y-axis), with the points colored by the number of bedrooms.\nMake sure the plot is interactive (zoom-able and pan-able) and add a tooltip with all of the columns in our scraped data frame.\nWith this sort of plot, you can quickly see the outlier apartments in terms of size and price."
  },
  {
    "objectID": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4.html#load-data-google-review-data",
    "href": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4.html#load-data-google-review-data",
    "title": "Assignment 4: Street Networks & Web Scraping & Text Analytics",
    "section": "3.1. Load Data Google Review Data",
    "text": "3.1. Load Data Google Review Data\nTo begin, let’s load a dataset of Google reviews for Wissahickon Valley Park. This dataset includes user reviews, ratings, and other information, which we’ll use to analyze visitors’ emotions.\n\nimport pandas as pd\n\n\ndat = pd.read_csv('wissahickon_review.csv')\ndat.head()"
  },
  {
    "objectID": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4.html#apply-emotion-analysis-model",
    "href": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4.html#apply-emotion-analysis-model",
    "title": "Assignment 4: Street Networks & Web Scraping & Text Analytics",
    "section": "3.2. Apply Emotion Analysis Model",
    "text": "3.2. Apply Emotion Analysis Model\nNow, let’s apply an emotion analysis model to understand the emotions conveyed in the reviews. We’ll use a pre-trained model from Hugging Face.\nNote1: Use the link below to access the model or choose another from Hugging Face if you prefer.\nDistilBERT Emotion Model\nNote2: take the label with the highest score as the predicted label for each text.\n\nfrom transformers import pipeline"
  },
  {
    "objectID": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4.html#visualize-the-emotion-breakdown",
    "href": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4.html#visualize-the-emotion-breakdown",
    "title": "Assignment 4: Street Networks & Web Scraping & Text Analytics",
    "section": "3.3. Visualize the Emotion Breakdown",
    "text": "3.3. Visualize the Emotion Breakdown\nTo gain insights, let’s visualize the breakdown of emotions expressed in the reviews. This will help us understand the general sentiment of park visitors and any prevailing emotions."
  },
  {
    "objectID": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4.html#additional-exploration",
    "href": "docs/24fall-python-assignment4-riya_varun_assign4/assignment-4.html#additional-exploration",
    "title": "Assignment 4: Street Networks & Web Scraping & Text Analytics",
    "section": "3.4. Additional Exploration",
    "text": "3.4. Additional Exploration\nYou can explore the data further by creating a word cloud of frequently mentioned words for a specific emotion. For example, if you want to focus on reviews with “joy” as the dominant emotion, what are the words that appear frequently in those reviews?"
  },
  {
    "objectID": "docs/24fall-python-assignment4-riya_varun_assign4/rubric.html",
    "href": "docs/24fall-python-assignment4-riya_varun_assign4/rubric.html",
    "title": "Grading Rubric for Assignment #4",
    "section": "",
    "text": "Please include your name at the top of the submitted notebook.\nImportant: Your notebooks should be a polished finished product. For example:\n\nplease remove any extra or unnecessary code.\nplease try to use markdown cells with section headers to mark different sections of the analysis.\n\n\n\nPart 1: Visualizing crash data in Philadelphia (20 points)\n\n1.1 Load the geometry for the region being analyzed (2 points)\n1.2 Get the street network graph (2 points)\n1.3 Convert your network graph edges to a GeoDataFrame (2 points)\n1.4 Load PennDOT crash data (1 point)\n1.5 Convert the crash data to a GeoDataFrame (1 point)\n1.6 Trim the crash data to Center City (2 points)\n1.7 Find the nearest edge for each crash (2 points)\n1.8 Calculate the total number of crashes per street (2 points)\n1.9 Merge your edges GeoDataFrame and crash count DataFrame (2 points)\n1.10 Calculate a “Crash Index” (2 points)\n1.11 Plot a histogram of the crash index values (1 point)\n1.12 Plot an interactive map of the street network, colored by the crash index (1 point)\n\nPart 2: Scraping Craigslist (20 points)\n\n2.1: Initialize a selenium driver and open Craigslist (1 point)\n2.2: Initialize your “soup” (1 point)\n2.3: Parsing the HTML (2 points)\n2.4 Find the relevant pieces of information (4 points)\n2.5 Functions to format the results (4 points)\n2.6: Putting it all together (4 points)\n2.7: Plotting rental prices (2 points)\n2.8: Comparing prices for different sizes (2 points)\n\nPart 3: Text Analytics (EXTRA 8 points)\n\n3.1. Load Data Google Review Data (1 point)\n3.2. Apply Emotion Analysis Model (2 points)\n3.3. Visualize the Emotion Breakdown (3 points)\n3.4. Additional Exploration (2 points)\n\nTotal points: 40 points"
  },
  {
    "objectID": "docs/24fall-python-assignment4-riya_varun_assign4/rubric.html#rubric",
    "href": "docs/24fall-python-assignment4-riya_varun_assign4/rubric.html#rubric",
    "title": "Grading Rubric for Assignment #4",
    "section": "",
    "text": "Part 1: Visualizing crash data in Philadelphia (20 points)\n\n1.1 Load the geometry for the region being analyzed (2 points)\n1.2 Get the street network graph (2 points)\n1.3 Convert your network graph edges to a GeoDataFrame (2 points)\n1.4 Load PennDOT crash data (1 point)\n1.5 Convert the crash data to a GeoDataFrame (1 point)\n1.6 Trim the crash data to Center City (2 points)\n1.7 Find the nearest edge for each crash (2 points)\n1.8 Calculate the total number of crashes per street (2 points)\n1.9 Merge your edges GeoDataFrame and crash count DataFrame (2 points)\n1.10 Calculate a “Crash Index” (2 points)\n1.11 Plot a histogram of the crash index values (1 point)\n1.12 Plot an interactive map of the street network, colored by the crash index (1 point)\n\nPart 2: Scraping Craigslist (20 points)\n\n2.1: Initialize a selenium driver and open Craigslist (1 point)\n2.2: Initialize your “soup” (1 point)\n2.3: Parsing the HTML (2 points)\n2.4 Find the relevant pieces of information (4 points)\n2.5 Functions to format the results (4 points)\n2.6: Putting it all together (4 points)\n2.7: Plotting rental prices (2 points)\n2.8: Comparing prices for different sizes (2 points)\n\nPart 3: Text Analytics (EXTRA 8 points)\n\n3.1. Load Data Google Review Data (1 point)\n3.2. Apply Emotion Analysis Model (2 points)\n3.3. Visualize the Emotion Breakdown (3 points)\n3.4. Additional Exploration (2 points)\n\nTotal points: 40 points"
  },
  {
    "objectID": "GTFS1226.html",
    "href": "GTFS1226.html",
    "title": "Mapping Transit Network",
    "section": "",
    "text": "#Load Packages:\n\nimport os \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport osmnx as ox\nimport shapely\nfrom shapely.geometry import Point\nimport folium\nfrom folium import Choropleth"
  },
  {
    "objectID": "GTFS1226.html#level-of-service-analysis",
    "href": "GTFS1226.html#level-of-service-analysis",
    "title": "Mapping Transit Network",
    "section": "Level of Service Analysis",
    "text": "Level of Service Analysis\n\nFrequency by Bus Stop\nTo evaluate the level of service across Philadelphia and understand user patterns, we visualized three key components—frequency, headway, and ridership—on a map for peak periods. The map reveals that most areas in the city benefit from frequent bus services, with frequency under 10 during peak times. The frequency of services seems to differ by bus routes rather than spatially.\n\nfig, ax = plt.subplots(figsize=(10, 8))\npeak_metrics_geo.plot(\n    ax=ax,\n    column=\"frequency\",  \n    cmap=\"plasma\",       \n    legend=True,           \n    markersize=2)\n\nplt.title(\"Peak Period Arrivals per hour (frequency) by Bus Stop\", fontsize=16)\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\n\nText(117.75660018496485, 0.5, 'Latitude')\n\n\n\n\n\n\n\n\n\n\n\nHeadway by Bus Stop\nHeadway patterns across Philadelphia highlight disparities in service consistency. The average headway during peak periods is under 20 minutes for most routes, indicating moderate accessibility for the majority of transit users. However, there are extreme outliers in some regions, where headway exceed 180 minutes, highlighting disparities in service provision. By adding a geographic component, we observe a more nuanced distribution of ridership across Philadelphia. As expected, areas in Center City exhibit high ridership and frequent bus service. Conversely, far-flung areas such as Upper Northwest and Lower Northwest show lower ridership levels and greater headway. Interestingly, Lower Northeast stands out with the highest ridership of all districts.\n\nfig, ax = plt.subplots(figsize=(10, 8))\npeak_metrics_geo.plot(\n    ax=ax,\n    column=\"headway\",  \n    cmap=\"plasma\",        \n    legend=True,           \n    markersize=2)\n\nplt.title(\"Headway (mins) by bus stop\", fontsize=16)\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\n\nText(117.75660018496485, 0.5, 'Latitude')\n\n\n\n\n\n\n\n\n\n\n\nRidership by Bus Stop\nRidership distribution presents a nuanced picture of bus service utilization across Philadelphia. While overall ridership appears relatively balanced, key trends emerge. Center City stands out with concentrated high ridership, reflecting its role as a hub for employment and activity. Meanwhile, areas like the Lower Northwest, near Mt. Airy, demonstrate higher ridership levels despite experiencing longer headway. Interestingly, the Lower Northeast exhibits the highest ridership among all districts, highlighting a unique demand for bus services in this region.\n\nfig, ax = plt.subplots(figsize=(10, 8))\npeak_metrics_geo.plot(\n    ax=ax,\n    column=\"Ridership\",  \n    cmap=\"plasma\",        \n    legend=True,         \n    markersize=2)\n\nplt.title(\"Daily Ridership by Bus Stop\", fontsize=16)\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\n\nText(117.75660018496485, 0.5, 'Latitude')"
  },
  {
    "objectID": "GTFS1226.html#spatial-join-bus-stops-to-bus-network",
    "href": "GTFS1226.html#spatial-join-bus-stops-to-bus-network",
    "title": "Mapping Transit Network",
    "section": "Spatial Join: Bus Stops to Bus Network",
    "text": "Spatial Join: Bus Stops to Bus Network\nTill now we were looking at transit data at a point, however to understand what routes generate the most traffic and what routes to prioritze, we need to look at the data at street level. To achieve this, we joined the bus stops to the bus network, ensuring that each point is associated with the nearest street segment.\n\n# Ensure both GeoDataFrames are in a projected CRS\npeak_metrics_geo = peak_metrics_geo.to_crs(\"EPSG: 2272\")\nedges_gdf = edges_gdf.to_crs(\"EPSG: 2272\")\npeak_metrics_geo = peak_metrics_geo.reset_index(drop=True)\n\n# Perform a spatial join to find the nearest road segment\npeak_metrics_network = gpd.sjoin_nearest(\n    edges_gdf,                 \n    peak_metrics_geo,\n    how=\"left\",               \n    distance_col=\"distance\")\n\n#Re-project to a geographic CRS\npeak_metrics_network = peak_metrics_network.to_crs(\"EPSG: 4326\")\npeak_metrics_network.head()\n\n\n\n\n\n\n\n\ngeometry\nstart\nend\nindex_right\nstop_id\nbus_arrivals\nfrequency\nheadway\nstop_name\nstop_lat\nstop_lon\nlocation_type\nparent_station\nzone_id\nwheelchair_boarding\nRidership\ndistance\n\n\n\n\n292\nLINESTRING (-75.25350 39.87616, -75.25399 39.8...\nPOINT (-75.253495 39.876162)\nPOINT (-75.25398839928764 39.87604214946382)\n0.0\n17103.0\n55.0\n18.333333\n3.272727\nPhiladelphia Airport Terminal A\n39.876761\n-75.247175\nNaN\n30615.0\n1.0\n1.0\n314.0\n1787.205503\n\n\n291\nLINESTRING (-75.25327 39.87622, -75.25350 39.8...\nPOINT (-75.25327 39.876224)\nPOINT (-75.253495 39.876162)\n0.0\n17103.0\n55.0\n18.333333\n3.272727\nPhiladelphia Airport Terminal A\n39.876761\n-75.247175\nNaN\n30615.0\n1.0\n1.0\n314.0\n1721.831860\n\n\n290\nLINESTRING (-75.25312 39.87627, -75.25327 39.8...\nPOINT (-75.253117 39.87627)\nPOINT (-75.25327 39.876224)\n0.0\n17103.0\n55.0\n18.333333\n3.272727\nPhiladelphia Airport Terminal A\n39.876761\n-75.247175\nNaN\n30615.0\n1.0\n1.0\n314.0\n1677.304469\n\n\n157\nLINESTRING (-75.25003 39.87626, -75.25003 39.8...\nPOINT (-75.2500277483444 39.87625617880794)\nPOINT (-75.250026 39.876257)\n0.0\n17103.0\n55.0\n18.333333\n3.272727\nPhiladelphia Airport Terminal A\n39.876761\n-75.247175\nNaN\n30615.0\n1.0\n1.0\n314.0\n820.984358\n\n\n293\nMULTILINESTRING ((-75.25399 39.87604, -75.2539...\nPOINT (-75.25398839928764 39.87604214946382)\nPOINT (-75.26120568475493 39.8764816048662)\n0.0\n17103.0\n55.0\n18.333333\n3.272727\nPhiladelphia Airport Terminal A\n39.876761\n-75.247175\nNaN\n30615.0\n1.0\n1.0\n314.0\n1930.164592\n\n\n\n\n\n\n\n\nVisualisation of Metrics by Street Segments on Philadelphia’s Bus Network\nFollowing the spatial join, we can now analyze bus headways during peak hours by street segments, providing a more refined view of service levels across different parts of the city. Major corridors in Philadelphia, such as Broad Street, Walnut Street, Chestnut Street, Market Street, and Roosevelt Boulevard, exhibit headways of under 20 minutes, reflecting frequent service. In contrast, streets on the city’s periphery have significantly longer headways, exceeding 120 minutes, highlighting areas with less frequent service. For our analysis we wanted to prioritize routes with the fastest headway, therefore the plot below only shows distribution of headway under 20 minutes.\n\npeak_metrics_network_filtered = peak_metrics_network[peak_metrics_network['headway'] &lt; 20]\n\nfig, ax = plt.subplots(figsize=(10, 8))\npeak_metrics_network_filtered.plot(\n    ax=ax,\n    column=\"headway\", \n    cmap=\"plasma\",        \n    legend=True,         \n    markersize=1)\n\nplt.title(\"Headway by Street Segment\", fontsize=16)\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\n\nText(109.11298115958094, 0.5, 'Latitude')\n\n\n\n\n\n\n\n\n\nWhen examining ridership by street segments, the distribution tells a different story. Ridership remains relatively consistent across most of the network, regardless of the headway. However, ridership spikes above 2,000 passengers for specific routes around Mt. Airy, Frankford, N Broad St., as well as Center City. These areas, particularly in the North, host major transportation hubs that attract a high volume of trips. As such a plot does not indicate variability in ridership, we decided to log transform the data. The map below now gives a more accurate picture, with ridership patterns mirroring the trends seen with the headway plot above. The ridership is the highest in Center City, followed by pockets of high ridership distributed evenly across the city.\n\npeak_metrics_network['Log_Ridership'] = np.log(peak_metrics_network['Ridership'] + 1)\n\nfig, ax = plt.subplots(figsize=(10, 8))\npeak_metrics_network.plot(\n    ax=ax,\n    column=\"Log_Ridership\",  \n    cmap=\"plasma\",       \n    legend=True,         \n    markersize=1)\n\nplt.title(\"Log(Ridership) by Street Segment\", fontsize=16)\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\n\nText(110.89550110614813, 0.5, 'Latitude')\n\n\n\n\n\n\n\n\n\n\npeak_metrics_network = peak_metrics_network.drop(columns=['start', 'end'], errors='ignore') \npeak_metrics_network.to_file(\"Data/peak_metrics_network.geojson\", driver=\"GeoJSON\")\n\n\nbus_network_philadelphia.to_file(\"Data/bus_network_philadelphia.geojson\", driver = \"GeoJSON\")"
  }
]